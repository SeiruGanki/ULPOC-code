{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af211f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edc5d553",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, N=1000,cut_off = 20, activation_type = None):\n",
    "        super(Net, self).__init__()\n",
    "        self.particle_num = N\n",
    "        \n",
    "        if activation_type == 'ReLU':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation_type == 'Sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "            \n",
    "        self.fc1 = nn.Linear(2, N)\n",
    "        self.fc2 = nn.Linear(N, 1, bias=False)\n",
    "        \n",
    "        nn.init.normal_(self.fc1.weight, mean=0, std=5)\n",
    "        nn.init.normal_(self.fc1.bias, mean=0, std=5)\n",
    "        nn.init.normal_(self.fc2.weight, mean=0, std=5)\n",
    "        \n",
    "        self.cut_off = cut_off\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        # restrict the weights of fc2 to be within [0, cutoff]\n",
    "        \n",
    "        if self.cut_off != None:\n",
    "            self.fc2.weight.data = torch.clamp(self.fc2.weight.data, min=-self.cut_off, max=self.cut_off)\n",
    "            \n",
    "        x = self.fc2(x)/self.particle_num \n",
    "        \n",
    "        return x.squeeze()\n",
    "\n",
    "class FullGradientDescentWithNoisyAndWeightDecay(torch.optim.Optimizer):\n",
    "    def __init__(self, params, N, lr=1e-3, weight_decay=0, noise_scale=0):\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay, noise_scale=noise_scale)\n",
    "        self.N =  N\n",
    "        super(FullGradientDescentWithNoisyAndWeightDecay, self).__init__(params, defaults)\n",
    "        \n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            weight_decay = group['weight_decay']\n",
    "            noise_scale = group['noise_scale']\n",
    "            \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                    \n",
    "                grad = p.grad.data * self.N\n",
    "                \n",
    "                if weight_decay != 0:\n",
    "                    grad.add_(weight_decay, p.data)\n",
    "                \n",
    "                p.data.add_(-lr, grad)\n",
    "                \n",
    "                if noise_scale != 0:\n",
    "                    noise = torch.randn_like(p.data) * noise_scale * (lr ** 1/2)\n",
    "                    p.data.add_(noise)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9a01254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_func(v):\n",
    "    return np.sin(2 * np.pi * v[:,0]) + np.cos(2 * np.pi * v[:,1])\n",
    "\n",
    "# Assuming you have input_data and target_data tensors\n",
    "input_data =  torch.rand(1000,2)  # shape: (num_samples, input_size)\n",
    "target_data =target_func(input_data)  # shape: (num_samples, target_size)\n",
    "\n",
    "# Create a TensorDataset object with your data tensors\n",
    "dataset = TensorDataset(input_data, target_data)\n",
    "\n",
    "def trainloss(x, y):\n",
    "    return (x - y) ** 2 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1359f4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset,\n",
    "    batch_size=1000, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    data, target = data.to(device), target.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24a4f78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 0.00001\n",
    "sigma = 1\n",
    "\n",
    "lr = 0.2 # KL lr = 0.5  MSE 0.1\n",
    "\n",
    "particle_num = 128\n",
    "cut_off = 100\n",
    "\n",
    "activation_type = 'Sigmoid' # relu as default\n",
    "loss_type = 'MSE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a5381c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train Loss: 0.471869 L2-Train Loss: 0.472851\n",
      "Epoch: 1000 Train Loss: 0.081808 L2-Train Loss: 0.083971\n",
      "Epoch: 2000 Train Loss: 0.025391 L2-Train Loss: 0.028876\n",
      "Epoch: 3000 Train Loss: 0.019669 L2-Train Loss: 0.023997\n",
      "Epoch: 4000 Train Loss: 0.013480 L2-Train Loss: 0.018792\n",
      "Epoch: 5000 Train Loss: 0.013781 L2-Train Loss: 0.019969\n",
      "Epoch: 6000 Train Loss: 0.012353 L2-Train Loss: 0.018957\n",
      "Epoch: 7000 Train Loss: 0.011965 L2-Train Loss: 0.018700\n",
      "Epoch: 8000 Train Loss: 0.013130 L2-Train Loss: 0.020174\n",
      "Epoch: 9000 Train Loss: 0.008548 L2-Train Loss: 0.016018\n",
      "Epoch: 10000 Train Loss: 0.007652 L2-Train Loss: 0.015290\n",
      "Epoch: 11000 Train Loss: 0.007442 L2-Train Loss: 0.015615\n",
      "Epoch: 12000 Train Loss: 0.006997 L2-Train Loss: 0.015609\n",
      "Epoch: 13000 Train Loss: 0.007480 L2-Train Loss: 0.016579\n",
      "Epoch: 14000 Train Loss: 0.007054 L2-Train Loss: 0.016688\n",
      "Epoch: 15000 Train Loss: 0.006558 L2-Train Loss: 0.016691\n",
      "Epoch: 16000 Train Loss: 0.007385 L2-Train Loss: 0.017823\n",
      "Epoch: 17000 Train Loss: 0.010263 L2-Train Loss: 0.020715\n",
      "Epoch: 18000 Train Loss: 0.009008 L2-Train Loss: 0.019976\n",
      "Epoch: 19000 Train Loss: 0.006845 L2-Train Loss: 0.018029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████▎                                                                          | 1/10 [00:52<07:49, 52.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.007449 L2-Train Loss: 0.019007\n",
      "Epoch: 1 Train Loss: 0.581867 L2-Train Loss: 0.582823\n",
      "Epoch: 1000 Train Loss: 0.068002 L2-Train Loss: 0.070321\n",
      "Epoch: 2000 Train Loss: 0.035251 L2-Train Loss: 0.038507\n",
      "Epoch: 3000 Train Loss: 0.020797 L2-Train Loss: 0.024775\n",
      "Epoch: 4000 Train Loss: 0.015126 L2-Train Loss: 0.019875\n",
      "Epoch: 5000 Train Loss: 0.016485 L2-Train Loss: 0.022043\n",
      "Epoch: 6000 Train Loss: 0.012484 L2-Train Loss: 0.018603\n",
      "Epoch: 7000 Train Loss: 0.011006 L2-Train Loss: 0.017650\n",
      "Epoch: 8000 Train Loss: 0.006813 L2-Train Loss: 0.013904\n",
      "Epoch: 9000 Train Loss: 0.010391 L2-Train Loss: 0.017621\n",
      "Epoch: 10000 Train Loss: 0.009216 L2-Train Loss: 0.016922\n",
      "Epoch: 11000 Train Loss: 0.008178 L2-Train Loss: 0.016768\n",
      "Epoch: 12000 Train Loss: 0.007109 L2-Train Loss: 0.016068\n",
      "Epoch: 13000 Train Loss: 0.008068 L2-Train Loss: 0.017332\n",
      "Epoch: 14000 Train Loss: 0.009974 L2-Train Loss: 0.020073\n",
      "Epoch: 15000 Train Loss: 0.004187 L2-Train Loss: 0.014893\n",
      "Epoch: 16000 Train Loss: 0.007782 L2-Train Loss: 0.018873\n",
      "Epoch: 17000 Train Loss: 0.006954 L2-Train Loss: 0.018618\n",
      "Epoch: 18000 Train Loss: 0.007030 L2-Train Loss: 0.018945\n",
      "Epoch: 19000 Train Loss: 0.007666 L2-Train Loss: 0.019743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▌                                                                  | 2/10 [01:44<06:56, 52.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.007068 L2-Train Loss: 0.019174\n",
      "Epoch: 1 Train Loss: 0.635605 L2-Train Loss: 0.636696\n",
      "Epoch: 1000 Train Loss: 0.090623 L2-Train Loss: 0.093070\n",
      "Epoch: 2000 Train Loss: 0.040048 L2-Train Loss: 0.043651\n",
      "Epoch: 3000 Train Loss: 0.022386 L2-Train Loss: 0.026694\n",
      "Epoch: 4000 Train Loss: 0.020338 L2-Train Loss: 0.025128\n",
      "Epoch: 5000 Train Loss: 0.018029 L2-Train Loss: 0.023185\n",
      "Epoch: 6000 Train Loss: 0.015307 L2-Train Loss: 0.020956\n",
      "Epoch: 7000 Train Loss: 0.011557 L2-Train Loss: 0.018023\n",
      "Epoch: 8000 Train Loss: 0.010617 L2-Train Loss: 0.017862\n",
      "Epoch: 9000 Train Loss: 0.007555 L2-Train Loss: 0.015423\n",
      "Epoch: 10000 Train Loss: 0.009112 L2-Train Loss: 0.017426\n",
      "Epoch: 11000 Train Loss: 0.010132 L2-Train Loss: 0.019039\n",
      "Epoch: 12000 Train Loss: 0.008461 L2-Train Loss: 0.017570\n",
      "Epoch: 13000 Train Loss: 0.008432 L2-Train Loss: 0.018042\n",
      "Epoch: 14000 Train Loss: 0.007449 L2-Train Loss: 0.017218\n",
      "Epoch: 15000 Train Loss: 0.010589 L2-Train Loss: 0.020908\n",
      "Epoch: 16000 Train Loss: 0.008813 L2-Train Loss: 0.019450\n",
      "Epoch: 17000 Train Loss: 0.009977 L2-Train Loss: 0.021184\n",
      "Epoch: 18000 Train Loss: 0.008719 L2-Train Loss: 0.020488\n",
      "Epoch: 19000 Train Loss: 0.009365 L2-Train Loss: 0.021742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████▉                                                          | 3/10 [02:35<06:02, 51.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.012695 L2-Train Loss: 0.024759\n",
      "Epoch: 1 Train Loss: 0.506730 L2-Train Loss: 0.507764\n",
      "Epoch: 1000 Train Loss: 0.060315 L2-Train Loss: 0.062635\n",
      "Epoch: 2000 Train Loss: 0.026187 L2-Train Loss: 0.029489\n",
      "Epoch: 3000 Train Loss: 0.015779 L2-Train Loss: 0.019849\n",
      "Epoch: 4000 Train Loss: 0.014859 L2-Train Loss: 0.019549\n",
      "Epoch: 5000 Train Loss: 0.014010 L2-Train Loss: 0.019144\n",
      "Epoch: 6000 Train Loss: 0.007449 L2-Train Loss: 0.013184\n",
      "Epoch: 7000 Train Loss: 0.007354 L2-Train Loss: 0.013538\n",
      "Epoch: 8000 Train Loss: 0.005470 L2-Train Loss: 0.012418\n",
      "Epoch: 9000 Train Loss: 0.007562 L2-Train Loss: 0.014909\n",
      "Epoch: 10000 Train Loss: 0.008492 L2-Train Loss: 0.016309\n",
      "Epoch: 11000 Train Loss: 0.007149 L2-Train Loss: 0.014922\n",
      "Epoch: 12000 Train Loss: 0.005706 L2-Train Loss: 0.013890\n",
      "Epoch: 13000 Train Loss: 0.006909 L2-Train Loss: 0.015673\n",
      "Epoch: 14000 Train Loss: 0.007277 L2-Train Loss: 0.016547\n",
      "Epoch: 15000 Train Loss: 0.005746 L2-Train Loss: 0.015168\n",
      "Epoch: 16000 Train Loss: 0.007943 L2-Train Loss: 0.017654\n",
      "Epoch: 17000 Train Loss: 0.006823 L2-Train Loss: 0.016673\n",
      "Epoch: 18000 Train Loss: 0.005784 L2-Train Loss: 0.015724\n",
      "Epoch: 19000 Train Loss: 0.007584 L2-Train Loss: 0.018478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▏                                                 | 4/10 [03:29<05:14, 52.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.008834 L2-Train Loss: 0.019732\n",
      "Epoch: 1 Train Loss: 0.729465 L2-Train Loss: 0.730435\n",
      "Epoch: 1000 Train Loss: 0.064339 L2-Train Loss: 0.066775\n",
      "Epoch: 2000 Train Loss: 0.029392 L2-Train Loss: 0.032759\n",
      "Epoch: 3000 Train Loss: 0.018558 L2-Train Loss: 0.022848\n",
      "Epoch: 4000 Train Loss: 0.013303 L2-Train Loss: 0.018469\n",
      "Epoch: 5000 Train Loss: 0.013081 L2-Train Loss: 0.019134\n",
      "Epoch: 6000 Train Loss: 0.009212 L2-Train Loss: 0.015631\n",
      "Epoch: 7000 Train Loss: 0.009510 L2-Train Loss: 0.016903\n",
      "Epoch: 8000 Train Loss: 0.007929 L2-Train Loss: 0.015924\n",
      "Epoch: 9000 Train Loss: 0.006815 L2-Train Loss: 0.015052\n",
      "Epoch: 10000 Train Loss: 0.006347 L2-Train Loss: 0.015232\n",
      "Epoch: 11000 Train Loss: 0.006301 L2-Train Loss: 0.015790\n",
      "Epoch: 12000 Train Loss: 0.007740 L2-Train Loss: 0.017981\n",
      "Epoch: 13000 Train Loss: 0.010927 L2-Train Loss: 0.021360\n",
      "Epoch: 14000 Train Loss: 0.006242 L2-Train Loss: 0.017072\n",
      "Epoch: 15000 Train Loss: 0.006582 L2-Train Loss: 0.017774\n",
      "Epoch: 16000 Train Loss: 0.007760 L2-Train Loss: 0.019408\n",
      "Epoch: 17000 Train Loss: 0.007185 L2-Train Loss: 0.019398\n",
      "Epoch: 18000 Train Loss: 0.007025 L2-Train Loss: 0.019951\n",
      "Epoch: 19000 Train Loss: 0.006255 L2-Train Loss: 0.019500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [04:21<04:22, 52.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.008274 L2-Train Loss: 0.021825\n",
      "Epoch: 1 Train Loss: 0.427526 L2-Train Loss: 0.428426\n",
      "Epoch: 1000 Train Loss: 0.109685 L2-Train Loss: 0.111703\n",
      "Epoch: 2000 Train Loss: 0.023958 L2-Train Loss: 0.027210\n",
      "Epoch: 3000 Train Loss: 0.015823 L2-Train Loss: 0.019883\n",
      "Epoch: 4000 Train Loss: 0.015186 L2-Train Loss: 0.019873\n",
      "Epoch: 5000 Train Loss: 0.014989 L2-Train Loss: 0.020312\n",
      "Epoch: 6000 Train Loss: 0.013061 L2-Train Loss: 0.018790\n",
      "Epoch: 7000 Train Loss: 0.010014 L2-Train Loss: 0.016497\n",
      "Epoch: 8000 Train Loss: 0.010994 L2-Train Loss: 0.017567\n",
      "Epoch: 9000 Train Loss: 0.008848 L2-Train Loss: 0.015872\n",
      "Epoch: 10000 Train Loss: 0.007266 L2-Train Loss: 0.014582\n",
      "Epoch: 11000 Train Loss: 0.005718 L2-Train Loss: 0.013830\n",
      "Epoch: 12000 Train Loss: 0.007169 L2-Train Loss: 0.015683\n",
      "Epoch: 13000 Train Loss: 0.008310 L2-Train Loss: 0.017336\n",
      "Epoch: 14000 Train Loss: 0.009321 L2-Train Loss: 0.018694\n",
      "Epoch: 15000 Train Loss: 0.008391 L2-Train Loss: 0.018457\n",
      "Epoch: 16000 Train Loss: 0.008338 L2-Train Loss: 0.018580\n",
      "Epoch: 17000 Train Loss: 0.010685 L2-Train Loss: 0.021263\n",
      "Epoch: 18000 Train Loss: 0.008201 L2-Train Loss: 0.019258\n",
      "Epoch: 19000 Train Loss: 0.008672 L2-Train Loss: 0.020550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [05:13<03:28, 52.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.009638 L2-Train Loss: 0.021815\n",
      "Epoch: 1 Train Loss: 0.573967 L2-Train Loss: 0.574980\n",
      "Epoch: 1000 Train Loss: 0.077662 L2-Train Loss: 0.080073\n",
      "Epoch: 2000 Train Loss: 0.029196 L2-Train Loss: 0.032797\n",
      "Epoch: 3000 Train Loss: 0.021407 L2-Train Loss: 0.025618\n",
      "Epoch: 4000 Train Loss: 0.022284 L2-Train Loss: 0.027088\n",
      "Epoch: 5000 Train Loss: 0.021695 L2-Train Loss: 0.026900\n",
      "Epoch: 6000 Train Loss: 0.017478 L2-Train Loss: 0.023457\n",
      "Epoch: 7000 Train Loss: 0.013763 L2-Train Loss: 0.020536\n",
      "Epoch: 8000 Train Loss: 0.014486 L2-Train Loss: 0.021872\n",
      "Epoch: 9000 Train Loss: 0.008098 L2-Train Loss: 0.015922\n",
      "Epoch: 10000 Train Loss: 0.007406 L2-Train Loss: 0.015542\n",
      "Epoch: 11000 Train Loss: 0.009799 L2-Train Loss: 0.018231\n",
      "Epoch: 12000 Train Loss: 0.007658 L2-Train Loss: 0.016223\n",
      "Epoch: 13000 Train Loss: 0.007478 L2-Train Loss: 0.016576\n",
      "Epoch: 14000 Train Loss: 0.005651 L2-Train Loss: 0.015304\n",
      "Epoch: 15000 Train Loss: 0.005318 L2-Train Loss: 0.015350\n",
      "Epoch: 16000 Train Loss: 0.006480 L2-Train Loss: 0.017152\n",
      "Epoch: 17000 Train Loss: 0.005145 L2-Train Loss: 0.015986\n",
      "Epoch: 18000 Train Loss: 0.005969 L2-Train Loss: 0.017071\n",
      "Epoch: 19000 Train Loss: 0.007623 L2-Train Loss: 0.019068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [06:04<02:35, 51.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.008173 L2-Train Loss: 0.020000\n",
      "Epoch: 1 Train Loss: 0.538153 L2-Train Loss: 0.539000\n",
      "Epoch: 1000 Train Loss: 0.060432 L2-Train Loss: 0.062660\n",
      "Epoch: 2000 Train Loss: 0.027842 L2-Train Loss: 0.031183\n",
      "Epoch: 3000 Train Loss: 0.021507 L2-Train Loss: 0.025724\n",
      "Epoch: 4000 Train Loss: 0.014516 L2-Train Loss: 0.019316\n",
      "Epoch: 5000 Train Loss: 0.015478 L2-Train Loss: 0.021013\n",
      "Epoch: 6000 Train Loss: 0.010834 L2-Train Loss: 0.016762\n",
      "Epoch: 7000 Train Loss: 0.008991 L2-Train Loss: 0.015440\n",
      "Epoch: 8000 Train Loss: 0.010393 L2-Train Loss: 0.017268\n",
      "Epoch: 9000 Train Loss: 0.010774 L2-Train Loss: 0.017982\n",
      "Epoch: 10000 Train Loss: 0.009180 L2-Train Loss: 0.016676\n",
      "Epoch: 11000 Train Loss: 0.006711 L2-Train Loss: 0.015078\n",
      "Epoch: 12000 Train Loss: 0.007242 L2-Train Loss: 0.015621\n",
      "Epoch: 13000 Train Loss: 0.006309 L2-Train Loss: 0.014756\n",
      "Epoch: 14000 Train Loss: 0.011216 L2-Train Loss: 0.020207\n",
      "Epoch: 15000 Train Loss: 0.007369 L2-Train Loss: 0.016680\n",
      "Epoch: 16000 Train Loss: 0.008295 L2-Train Loss: 0.018223\n",
      "Epoch: 17000 Train Loss: 0.006793 L2-Train Loss: 0.017192\n",
      "Epoch: 18000 Train Loss: 0.006461 L2-Train Loss: 0.017247\n",
      "Epoch: 19000 Train Loss: 0.006207 L2-Train Loss: 0.017274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [06:56<01:43, 51.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.006021 L2-Train Loss: 0.017723\n",
      "Epoch: 1 Train Loss: 0.508220 L2-Train Loss: 0.509303\n",
      "Epoch: 1000 Train Loss: 0.082547 L2-Train Loss: 0.085016\n",
      "Epoch: 2000 Train Loss: 0.027047 L2-Train Loss: 0.030651\n",
      "Epoch: 3000 Train Loss: 0.018012 L2-Train Loss: 0.022360\n",
      "Epoch: 4000 Train Loss: 0.018245 L2-Train Loss: 0.023192\n",
      "Epoch: 5000 Train Loss: 0.011072 L2-Train Loss: 0.016662\n",
      "Epoch: 6000 Train Loss: 0.012631 L2-Train Loss: 0.018602\n",
      "Epoch: 7000 Train Loss: 0.010312 L2-Train Loss: 0.016739\n",
      "Epoch: 8000 Train Loss: 0.012558 L2-Train Loss: 0.019441\n",
      "Epoch: 9000 Train Loss: 0.009017 L2-Train Loss: 0.016370\n",
      "Epoch: 10000 Train Loss: 0.007210 L2-Train Loss: 0.015511\n",
      "Epoch: 11000 Train Loss: 0.005912 L2-Train Loss: 0.014164\n",
      "Epoch: 12000 Train Loss: 0.010977 L2-Train Loss: 0.019385\n",
      "Epoch: 13000 Train Loss: 0.007668 L2-Train Loss: 0.016461\n",
      "Epoch: 14000 Train Loss: 0.006968 L2-Train Loss: 0.016480\n",
      "Epoch: 15000 Train Loss: 0.007257 L2-Train Loss: 0.017480\n",
      "Epoch: 16000 Train Loss: 0.005950 L2-Train Loss: 0.016747\n",
      "Epoch: 17000 Train Loss: 0.008649 L2-Train Loss: 0.019775\n",
      "Epoch: 18000 Train Loss: 0.008179 L2-Train Loss: 0.019507\n",
      "Epoch: 19000 Train Loss: 0.009055 L2-Train Loss: 0.020545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [07:47<00:51, 51.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.008552 L2-Train Loss: 0.020384\n",
      "Epoch: 1 Train Loss: 0.612023 L2-Train Loss: 0.612840\n",
      "Epoch: 1000 Train Loss: 0.089747 L2-Train Loss: 0.091847\n",
      "Epoch: 2000 Train Loss: 0.031198 L2-Train Loss: 0.034397\n",
      "Epoch: 3000 Train Loss: 0.018717 L2-Train Loss: 0.022747\n",
      "Epoch: 4000 Train Loss: 0.016562 L2-Train Loss: 0.021279\n",
      "Epoch: 5000 Train Loss: 0.017283 L2-Train Loss: 0.022385\n",
      "Epoch: 6000 Train Loss: 0.014501 L2-Train Loss: 0.019831\n",
      "Epoch: 7000 Train Loss: 0.012238 L2-Train Loss: 0.018046\n",
      "Epoch: 8000 Train Loss: 0.013703 L2-Train Loss: 0.020172\n",
      "Epoch: 9000 Train Loss: 0.010892 L2-Train Loss: 0.018089\n",
      "Epoch: 10000 Train Loss: 0.006109 L2-Train Loss: 0.013669\n",
      "Epoch: 11000 Train Loss: 0.012041 L2-Train Loss: 0.019878\n",
      "Epoch: 12000 Train Loss: 0.009149 L2-Train Loss: 0.017572\n",
      "Epoch: 13000 Train Loss: 0.005818 L2-Train Loss: 0.014717\n",
      "Epoch: 14000 Train Loss: 0.006450 L2-Train Loss: 0.015507\n",
      "Epoch: 15000 Train Loss: 0.005536 L2-Train Loss: 0.015036\n",
      "Epoch: 16000 Train Loss: 0.006516 L2-Train Loss: 0.016099\n",
      "Epoch: 17000 Train Loss: 0.006337 L2-Train Loss: 0.016227\n",
      "Epoch: 18000 Train Loss: 0.005436 L2-Train Loss: 0.015634\n",
      "Epoch: 19000 Train Loss: 0.008215 L2-Train Loss: 0.018515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [08:40<00:00, 52.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.005432 L2-Train Loss: 0.015935\n",
      "Finish test particle_num=64 cutoff = 100.00 avg_trainloss=7.970e-03 avg_L2_trainloss=1.977e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train Loss: 0.571520 L2-Train Loss: 0.572560\n",
      "Epoch: 1000 Train Loss: 0.084806 L2-Train Loss: 0.087080\n",
      "Epoch: 2000 Train Loss: 0.028942 L2-Train Loss: 0.032342\n",
      "Epoch: 3000 Train Loss: 0.017880 L2-Train Loss: 0.022129\n",
      "Epoch: 4000 Train Loss: 0.014086 L2-Train Loss: 0.019030\n",
      "Epoch: 5000 Train Loss: 0.009776 L2-Train Loss: 0.015285\n",
      "Epoch: 6000 Train Loss: 0.007721 L2-Train Loss: 0.013918\n",
      "Epoch: 7000 Train Loss: 0.006705 L2-Train Loss: 0.013194\n",
      "Epoch: 8000 Train Loss: 0.004596 L2-Train Loss: 0.011651\n",
      "Epoch: 9000 Train Loss: 0.004297 L2-Train Loss: 0.011799\n",
      "Epoch: 10000 Train Loss: 0.004860 L2-Train Loss: 0.012890\n",
      "Epoch: 11000 Train Loss: 0.004675 L2-Train Loss: 0.013452\n",
      "Epoch: 12000 Train Loss: 0.004613 L2-Train Loss: 0.013938\n",
      "Epoch: 13000 Train Loss: 0.006733 L2-Train Loss: 0.016669\n",
      "Epoch: 14000 Train Loss: 0.005133 L2-Train Loss: 0.015351\n",
      "Epoch: 15000 Train Loss: 0.005302 L2-Train Loss: 0.016014\n",
      "Epoch: 16000 Train Loss: 0.004435 L2-Train Loss: 0.015679\n",
      "Epoch: 17000 Train Loss: 0.003083 L2-Train Loss: 0.014681\n",
      "Epoch: 18000 Train Loss: 0.005098 L2-Train Loss: 0.016957\n",
      "Epoch: 19000 Train Loss: 0.004765 L2-Train Loss: 0.017210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████▎                                                                          | 1/10 [00:51<07:45, 51.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.004488 L2-Train Loss: 0.017250\n",
      "Epoch: 1 Train Loss: 0.652482 L2-Train Loss: 0.653451\n",
      "Epoch: 1000 Train Loss: 0.080200 L2-Train Loss: 0.082461\n",
      "Epoch: 2000 Train Loss: 0.030341 L2-Train Loss: 0.033571\n",
      "Epoch: 3000 Train Loss: 0.019358 L2-Train Loss: 0.023402\n",
      "Epoch: 4000 Train Loss: 0.017235 L2-Train Loss: 0.021899\n",
      "Epoch: 5000 Train Loss: 0.011938 L2-Train Loss: 0.017673\n",
      "Epoch: 6000 Train Loss: 0.009158 L2-Train Loss: 0.015392\n",
      "Epoch: 7000 Train Loss: 0.010026 L2-Train Loss: 0.016860\n",
      "Epoch: 8000 Train Loss: 0.005659 L2-Train Loss: 0.013035\n",
      "Epoch: 9000 Train Loss: 0.005069 L2-Train Loss: 0.012816\n",
      "Epoch: 10000 Train Loss: 0.006887 L2-Train Loss: 0.015153\n",
      "Epoch: 11000 Train Loss: 0.004976 L2-Train Loss: 0.013731\n",
      "Epoch: 12000 Train Loss: 0.005029 L2-Train Loss: 0.014395\n",
      "Epoch: 13000 Train Loss: 0.004581 L2-Train Loss: 0.014335\n",
      "Epoch: 14000 Train Loss: 0.004643 L2-Train Loss: 0.014494\n",
      "Epoch: 15000 Train Loss: 0.004578 L2-Train Loss: 0.014741\n",
      "Epoch: 16000 Train Loss: 0.004842 L2-Train Loss: 0.015471\n",
      "Epoch: 17000 Train Loss: 0.003908 L2-Train Loss: 0.014940\n",
      "Epoch: 18000 Train Loss: 0.005024 L2-Train Loss: 0.016739\n",
      "Epoch: 19000 Train Loss: 0.005280 L2-Train Loss: 0.017724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▌                                                                  | 2/10 [01:43<06:52, 51.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.005355 L2-Train Loss: 0.018344\n",
      "Epoch: 1 Train Loss: 0.586148 L2-Train Loss: 0.587134\n",
      "Epoch: 1000 Train Loss: 0.078250 L2-Train Loss: 0.080596\n",
      "Epoch: 2000 Train Loss: 0.021046 L2-Train Loss: 0.024638\n",
      "Epoch: 3000 Train Loss: 0.014745 L2-Train Loss: 0.019016\n",
      "Epoch: 4000 Train Loss: 0.011529 L2-Train Loss: 0.016340\n",
      "Epoch: 5000 Train Loss: 0.010423 L2-Train Loss: 0.015783\n",
      "Epoch: 6000 Train Loss: 0.007658 L2-Train Loss: 0.013477\n",
      "Epoch: 7000 Train Loss: 0.007714 L2-Train Loss: 0.014070\n",
      "Epoch: 8000 Train Loss: 0.005894 L2-Train Loss: 0.012732\n",
      "Epoch: 9000 Train Loss: 0.006473 L2-Train Loss: 0.014043\n",
      "Epoch: 10000 Train Loss: 0.004534 L2-Train Loss: 0.012710\n",
      "Epoch: 11000 Train Loss: 0.003494 L2-Train Loss: 0.012071\n",
      "Epoch: 12000 Train Loss: 0.005862 L2-Train Loss: 0.015175\n",
      "Epoch: 13000 Train Loss: 0.005025 L2-Train Loss: 0.014848\n",
      "Epoch: 14000 Train Loss: 0.005291 L2-Train Loss: 0.015329\n",
      "Epoch: 15000 Train Loss: 0.005020 L2-Train Loss: 0.015453\n",
      "Epoch: 16000 Train Loss: 0.003882 L2-Train Loss: 0.014652\n",
      "Epoch: 17000 Train Loss: 0.003952 L2-Train Loss: 0.015160\n",
      "Epoch: 18000 Train Loss: 0.004600 L2-Train Loss: 0.016167\n",
      "Epoch: 19000 Train Loss: 0.003122 L2-Train Loss: 0.015057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████▉                                                          | 3/10 [02:34<06:01, 51.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.003284 L2-Train Loss: 0.015302\n",
      "Epoch: 1 Train Loss: 0.562291 L2-Train Loss: 0.563337\n",
      "Epoch: 1000 Train Loss: 0.074085 L2-Train Loss: 0.076318\n",
      "Epoch: 2000 Train Loss: 0.026254 L2-Train Loss: 0.029576\n",
      "Epoch: 3000 Train Loss: 0.016339 L2-Train Loss: 0.020448\n",
      "Epoch: 4000 Train Loss: 0.015654 L2-Train Loss: 0.020311\n",
      "Epoch: 5000 Train Loss: 0.011962 L2-Train Loss: 0.017277\n",
      "Epoch: 6000 Train Loss: 0.009514 L2-Train Loss: 0.015282\n",
      "Epoch: 7000 Train Loss: 0.008058 L2-Train Loss: 0.014333\n",
      "Epoch: 8000 Train Loss: 0.007339 L2-Train Loss: 0.014265\n",
      "Epoch: 9000 Train Loss: 0.004888 L2-Train Loss: 0.012498\n",
      "Epoch: 10000 Train Loss: 0.004508 L2-Train Loss: 0.012519\n",
      "Epoch: 11000 Train Loss: 0.005883 L2-Train Loss: 0.014106\n",
      "Epoch: 12000 Train Loss: 0.005154 L2-Train Loss: 0.013946\n",
      "Epoch: 13000 Train Loss: 0.004584 L2-Train Loss: 0.013671\n",
      "Epoch: 14000 Train Loss: 0.005361 L2-Train Loss: 0.015094\n",
      "Epoch: 15000 Train Loss: 0.005009 L2-Train Loss: 0.015163\n",
      "Epoch: 16000 Train Loss: 0.005191 L2-Train Loss: 0.015620\n",
      "Epoch: 17000 Train Loss: 0.003145 L2-Train Loss: 0.014434\n",
      "Epoch: 18000 Train Loss: 0.006845 L2-Train Loss: 0.018690\n",
      "Epoch: 19000 Train Loss: 0.004065 L2-Train Loss: 0.016277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▏                                                 | 4/10 [03:26<05:09, 51.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.003946 L2-Train Loss: 0.016925\n",
      "Epoch: 1 Train Loss: 0.667122 L2-Train Loss: 0.668084\n",
      "Epoch: 1000 Train Loss: 0.069045 L2-Train Loss: 0.071439\n",
      "Epoch: 2000 Train Loss: 0.031050 L2-Train Loss: 0.034413\n",
      "Epoch: 3000 Train Loss: 0.019737 L2-Train Loss: 0.024001\n",
      "Epoch: 4000 Train Loss: 0.016243 L2-Train Loss: 0.021084\n",
      "Epoch: 5000 Train Loss: 0.012238 L2-Train Loss: 0.017613\n",
      "Epoch: 6000 Train Loss: 0.010490 L2-Train Loss: 0.016658\n",
      "Epoch: 7000 Train Loss: 0.009695 L2-Train Loss: 0.016331\n",
      "Epoch: 8000 Train Loss: 0.010183 L2-Train Loss: 0.017399\n",
      "Epoch: 9000 Train Loss: 0.007867 L2-Train Loss: 0.015692\n",
      "Epoch: 10000 Train Loss: 0.005753 L2-Train Loss: 0.013963\n",
      "Epoch: 11000 Train Loss: 0.004401 L2-Train Loss: 0.013268\n",
      "Epoch: 12000 Train Loss: 0.003564 L2-Train Loss: 0.012915\n",
      "Epoch: 13000 Train Loss: 0.003775 L2-Train Loss: 0.013571\n",
      "Epoch: 14000 Train Loss: 0.005889 L2-Train Loss: 0.016194\n",
      "Epoch: 15000 Train Loss: 0.004979 L2-Train Loss: 0.015587\n",
      "Epoch: 16000 Train Loss: 0.004680 L2-Train Loss: 0.015801\n",
      "Epoch: 17000 Train Loss: 0.005484 L2-Train Loss: 0.017031\n",
      "Epoch: 18000 Train Loss: 0.003733 L2-Train Loss: 0.015728\n",
      "Epoch: 19000 Train Loss: 0.004088 L2-Train Loss: 0.016739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [04:17<04:17, 51.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.004142 L2-Train Loss: 0.017007\n",
      "Epoch: 1 Train Loss: 0.547384 L2-Train Loss: 0.548339\n",
      "Epoch: 1000 Train Loss: 0.074387 L2-Train Loss: 0.076767\n",
      "Epoch: 2000 Train Loss: 0.032537 L2-Train Loss: 0.035895\n",
      "Epoch: 3000 Train Loss: 0.021872 L2-Train Loss: 0.026117\n",
      "Epoch: 4000 Train Loss: 0.017930 L2-Train Loss: 0.022788\n",
      "Epoch: 5000 Train Loss: 0.014521 L2-Train Loss: 0.019875\n",
      "Epoch: 6000 Train Loss: 0.012846 L2-Train Loss: 0.018744\n",
      "Epoch: 7000 Train Loss: 0.009938 L2-Train Loss: 0.016353\n",
      "Epoch: 8000 Train Loss: 0.006191 L2-Train Loss: 0.013163\n",
      "Epoch: 9000 Train Loss: 0.005145 L2-Train Loss: 0.012879\n",
      "Epoch: 10000 Train Loss: 0.004404 L2-Train Loss: 0.012286\n",
      "Epoch: 11000 Train Loss: 0.003829 L2-Train Loss: 0.011954\n",
      "Epoch: 12000 Train Loss: 0.004224 L2-Train Loss: 0.013019\n",
      "Epoch: 13000 Train Loss: 0.005993 L2-Train Loss: 0.015375\n",
      "Epoch: 14000 Train Loss: 0.006320 L2-Train Loss: 0.015959\n",
      "Epoch: 15000 Train Loss: 0.005700 L2-Train Loss: 0.015684\n",
      "Epoch: 16000 Train Loss: 0.003906 L2-Train Loss: 0.014238\n",
      "Epoch: 17000 Train Loss: 0.004397 L2-Train Loss: 0.014607\n",
      "Epoch: 18000 Train Loss: 0.004408 L2-Train Loss: 0.015215\n",
      "Epoch: 19000 Train Loss: 0.005322 L2-Train Loss: 0.016429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [05:09<03:25, 51.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.004328 L2-Train Loss: 0.016019\n",
      "Epoch: 1 Train Loss: 0.552933 L2-Train Loss: 0.553914\n",
      "Epoch: 1000 Train Loss: 0.088941 L2-Train Loss: 0.091186\n",
      "Epoch: 2000 Train Loss: 0.028839 L2-Train Loss: 0.032159\n",
      "Epoch: 3000 Train Loss: 0.018850 L2-Train Loss: 0.022908\n",
      "Epoch: 4000 Train Loss: 0.016254 L2-Train Loss: 0.020997\n",
      "Epoch: 5000 Train Loss: 0.012334 L2-Train Loss: 0.017715\n",
      "Epoch: 6000 Train Loss: 0.007499 L2-Train Loss: 0.013301\n",
      "Epoch: 7000 Train Loss: 0.009110 L2-Train Loss: 0.015473\n",
      "Epoch: 8000 Train Loss: 0.006505 L2-Train Loss: 0.013187\n",
      "Epoch: 9000 Train Loss: 0.007081 L2-Train Loss: 0.014333\n",
      "Epoch: 10000 Train Loss: 0.005779 L2-Train Loss: 0.013606\n",
      "Epoch: 11000 Train Loss: 0.007412 L2-Train Loss: 0.015643\n",
      "Epoch: 12000 Train Loss: 0.007063 L2-Train Loss: 0.015733\n",
      "Epoch: 13000 Train Loss: 0.006014 L2-Train Loss: 0.015459\n",
      "Epoch: 14000 Train Loss: 0.004606 L2-Train Loss: 0.014470\n",
      "Epoch: 15000 Train Loss: 0.005481 L2-Train Loss: 0.015790\n",
      "Epoch: 16000 Train Loss: 0.005232 L2-Train Loss: 0.015944\n",
      "Epoch: 17000 Train Loss: 0.005387 L2-Train Loss: 0.016553\n",
      "Epoch: 18000 Train Loss: 0.003310 L2-Train Loss: 0.014985\n",
      "Epoch: 19000 Train Loss: 0.006561 L2-Train Loss: 0.018566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [06:00<02:34, 51.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.005508 L2-Train Loss: 0.017930\n",
      "Epoch: 1 Train Loss: 0.525244 L2-Train Loss: 0.526287\n",
      "Epoch: 1000 Train Loss: 0.060827 L2-Train Loss: 0.063256\n",
      "Epoch: 2000 Train Loss: 0.025777 L2-Train Loss: 0.029312\n",
      "Epoch: 3000 Train Loss: 0.020166 L2-Train Loss: 0.024288\n",
      "Epoch: 4000 Train Loss: 0.016128 L2-Train Loss: 0.020861\n",
      "Epoch: 5000 Train Loss: 0.013472 L2-Train Loss: 0.018814\n",
      "Epoch: 6000 Train Loss: 0.011216 L2-Train Loss: 0.017177\n",
      "Epoch: 7000 Train Loss: 0.006311 L2-Train Loss: 0.012935\n",
      "Epoch: 8000 Train Loss: 0.006326 L2-Train Loss: 0.013719\n",
      "Epoch: 9000 Train Loss: 0.005781 L2-Train Loss: 0.013912\n",
      "Epoch: 10000 Train Loss: 0.005170 L2-Train Loss: 0.013872\n",
      "Epoch: 11000 Train Loss: 0.006150 L2-Train Loss: 0.015369\n",
      "Epoch: 12000 Train Loss: 0.005045 L2-Train Loss: 0.014813\n",
      "Epoch: 13000 Train Loss: 0.004534 L2-Train Loss: 0.014656\n",
      "Epoch: 14000 Train Loss: 0.004155 L2-Train Loss: 0.014730\n",
      "Epoch: 15000 Train Loss: 0.005989 L2-Train Loss: 0.016833\n",
      "Epoch: 16000 Train Loss: 0.003827 L2-Train Loss: 0.015147\n",
      "Epoch: 17000 Train Loss: 0.004277 L2-Train Loss: 0.015883\n",
      "Epoch: 18000 Train Loss: 0.006248 L2-Train Loss: 0.018388\n",
      "Epoch: 19000 Train Loss: 0.005768 L2-Train Loss: 0.018538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [06:52<01:43, 51.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.004693 L2-Train Loss: 0.017821\n",
      "Epoch: 1 Train Loss: 0.544190 L2-Train Loss: 0.545172\n",
      "Epoch: 1000 Train Loss: 0.077277 L2-Train Loss: 0.079636\n",
      "Epoch: 2000 Train Loss: 0.029565 L2-Train Loss: 0.032987\n",
      "Epoch: 3000 Train Loss: 0.020897 L2-Train Loss: 0.024960\n",
      "Epoch: 4000 Train Loss: 0.017771 L2-Train Loss: 0.022554\n",
      "Epoch: 5000 Train Loss: 0.012842 L2-Train Loss: 0.018286\n",
      "Epoch: 6000 Train Loss: 0.012778 L2-Train Loss: 0.018699\n",
      "Epoch: 7000 Train Loss: 0.012741 L2-Train Loss: 0.019030\n",
      "Epoch: 8000 Train Loss: 0.008467 L2-Train Loss: 0.015451\n",
      "Epoch: 9000 Train Loss: 0.007715 L2-Train Loss: 0.015199\n",
      "Epoch: 10000 Train Loss: 0.004744 L2-Train Loss: 0.012444\n",
      "Epoch: 11000 Train Loss: 0.007065 L2-Train Loss: 0.015567\n",
      "Epoch: 12000 Train Loss: 0.005907 L2-Train Loss: 0.014883\n",
      "Epoch: 13000 Train Loss: 0.004734 L2-Train Loss: 0.014124\n",
      "Epoch: 14000 Train Loss: 0.004056 L2-Train Loss: 0.013951\n",
      "Epoch: 15000 Train Loss: 0.005198 L2-Train Loss: 0.015617\n",
      "Epoch: 16000 Train Loss: 0.003828 L2-Train Loss: 0.014658\n",
      "Epoch: 17000 Train Loss: 0.005486 L2-Train Loss: 0.016913\n",
      "Epoch: 18000 Train Loss: 0.004672 L2-Train Loss: 0.016348\n",
      "Epoch: 19000 Train Loss: 0.004206 L2-Train Loss: 0.016295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [07:43<00:51, 51.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.005271 L2-Train Loss: 0.017706\n",
      "Epoch: 1 Train Loss: 0.502864 L2-Train Loss: 0.503745\n",
      "Epoch: 1000 Train Loss: 0.080608 L2-Train Loss: 0.082854\n",
      "Epoch: 2000 Train Loss: 0.030790 L2-Train Loss: 0.034040\n",
      "Epoch: 3000 Train Loss: 0.022492 L2-Train Loss: 0.026462\n",
      "Epoch: 4000 Train Loss: 0.016631 L2-Train Loss: 0.021300\n",
      "Epoch: 5000 Train Loss: 0.012369 L2-Train Loss: 0.017668\n",
      "Epoch: 6000 Train Loss: 0.010310 L2-Train Loss: 0.016190\n",
      "Epoch: 7000 Train Loss: 0.008567 L2-Train Loss: 0.014981\n",
      "Epoch: 8000 Train Loss: 0.006738 L2-Train Loss: 0.013844\n",
      "Epoch: 9000 Train Loss: 0.004945 L2-Train Loss: 0.012646\n",
      "Epoch: 10000 Train Loss: 0.004942 L2-Train Loss: 0.013232\n",
      "Epoch: 11000 Train Loss: 0.004220 L2-Train Loss: 0.012953\n",
      "Epoch: 12000 Train Loss: 0.002928 L2-Train Loss: 0.011867\n",
      "Epoch: 13000 Train Loss: 0.003078 L2-Train Loss: 0.012385\n",
      "Epoch: 14000 Train Loss: 0.003498 L2-Train Loss: 0.013286\n",
      "Epoch: 15000 Train Loss: 0.004731 L2-Train Loss: 0.014900\n",
      "Epoch: 16000 Train Loss: 0.003030 L2-Train Loss: 0.013417\n",
      "Epoch: 17000 Train Loss: 0.003932 L2-Train Loss: 0.014974\n",
      "Epoch: 18000 Train Loss: 0.003620 L2-Train Loss: 0.015189\n",
      "Epoch: 19000 Train Loss: 0.003817 L2-Train Loss: 0.015498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [08:35<00:00, 51.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.004153 L2-Train Loss: 0.016437\n",
      "Finish test particle_num=128 cutoff = 100.00 avg_trainloss=4.629e-03 avg_L2_trainloss=1.710e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train Loss: 0.571904 L2-Train Loss: 0.572873\n",
      "Epoch: 1000 Train Loss: 0.069225 L2-Train Loss: 0.071602\n",
      "Epoch: 2000 Train Loss: 0.027103 L2-Train Loss: 0.030512\n",
      "Epoch: 3000 Train Loss: 0.017849 L2-Train Loss: 0.021964\n",
      "Epoch: 4000 Train Loss: 0.014708 L2-Train Loss: 0.019451\n",
      "Epoch: 5000 Train Loss: 0.010870 L2-Train Loss: 0.016245\n",
      "Epoch: 6000 Train Loss: 0.008832 L2-Train Loss: 0.014626\n",
      "Epoch: 7000 Train Loss: 0.007816 L2-Train Loss: 0.014159\n",
      "Epoch: 8000 Train Loss: 0.005588 L2-Train Loss: 0.012540\n",
      "Epoch: 9000 Train Loss: 0.005102 L2-Train Loss: 0.012617\n",
      "Epoch: 10000 Train Loss: 0.004112 L2-Train Loss: 0.011964\n",
      "Epoch: 11000 Train Loss: 0.003215 L2-Train Loss: 0.011466\n",
      "Epoch: 12000 Train Loss: 0.003694 L2-Train Loss: 0.012401\n",
      "Epoch: 13000 Train Loss: 0.002232 L2-Train Loss: 0.011394\n",
      "Epoch: 14000 Train Loss: 0.002965 L2-Train Loss: 0.012526\n",
      "Epoch: 15000 Train Loss: 0.002956 L2-Train Loss: 0.012941\n",
      "Epoch: 16000 Train Loss: 0.003330 L2-Train Loss: 0.013518\n",
      "Epoch: 17000 Train Loss: 0.003221 L2-Train Loss: 0.013678\n",
      "Epoch: 18000 Train Loss: 0.003080 L2-Train Loss: 0.013883\n",
      "Epoch: 19000 Train Loss: 0.003370 L2-Train Loss: 0.014531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████▎                                                                          | 1/10 [00:51<07:45, 51.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.003153 L2-Train Loss: 0.014630\n",
      "Epoch: 1 Train Loss: 0.573103 L2-Train Loss: 0.574096\n",
      "Epoch: 1000 Train Loss: 0.072735 L2-Train Loss: 0.075073\n",
      "Epoch: 2000 Train Loss: 0.025203 L2-Train Loss: 0.028622\n",
      "Epoch: 3000 Train Loss: 0.017190 L2-Train Loss: 0.021468\n",
      "Epoch: 4000 Train Loss: 0.012659 L2-Train Loss: 0.017579\n",
      "Epoch: 5000 Train Loss: 0.008370 L2-Train Loss: 0.013924\n",
      "Epoch: 6000 Train Loss: 0.007443 L2-Train Loss: 0.013538\n",
      "Epoch: 7000 Train Loss: 0.006307 L2-Train Loss: 0.013144\n",
      "Epoch: 8000 Train Loss: 0.005125 L2-Train Loss: 0.012480\n",
      "Epoch: 9000 Train Loss: 0.004537 L2-Train Loss: 0.012270\n",
      "Epoch: 10000 Train Loss: 0.003613 L2-Train Loss: 0.011853\n",
      "Epoch: 11000 Train Loss: 0.003624 L2-Train Loss: 0.012518\n",
      "Epoch: 12000 Train Loss: 0.004657 L2-Train Loss: 0.013989\n",
      "Epoch: 13000 Train Loss: 0.003641 L2-Train Loss: 0.013561\n",
      "Epoch: 14000 Train Loss: 0.002062 L2-Train Loss: 0.012317\n",
      "Epoch: 15000 Train Loss: 0.002605 L2-Train Loss: 0.013050\n",
      "Epoch: 16000 Train Loss: 0.002453 L2-Train Loss: 0.013123\n",
      "Epoch: 17000 Train Loss: 0.002240 L2-Train Loss: 0.013255\n",
      "Epoch: 18000 Train Loss: 0.002674 L2-Train Loss: 0.014143\n",
      "Epoch: 19000 Train Loss: 0.003321 L2-Train Loss: 0.015065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▌                                                                  | 2/10 [01:42<06:51, 51.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.004225 L2-Train Loss: 0.016500\n",
      "Epoch: 1 Train Loss: 0.587108 L2-Train Loss: 0.588076\n",
      "Epoch: 1000 Train Loss: 0.061951 L2-Train Loss: 0.064312\n",
      "Epoch: 2000 Train Loss: 0.027199 L2-Train Loss: 0.030669\n",
      "Epoch: 3000 Train Loss: 0.014850 L2-Train Loss: 0.019223\n",
      "Epoch: 4000 Train Loss: 0.012556 L2-Train Loss: 0.017523\n",
      "Epoch: 5000 Train Loss: 0.010008 L2-Train Loss: 0.015555\n",
      "Epoch: 6000 Train Loss: 0.009086 L2-Train Loss: 0.015064\n",
      "Epoch: 7000 Train Loss: 0.006586 L2-Train Loss: 0.013138\n",
      "Epoch: 8000 Train Loss: 0.007087 L2-Train Loss: 0.014193\n",
      "Epoch: 9000 Train Loss: 0.005513 L2-Train Loss: 0.013173\n",
      "Epoch: 10000 Train Loss: 0.004692 L2-Train Loss: 0.012974\n",
      "Epoch: 11000 Train Loss: 0.003680 L2-Train Loss: 0.012437\n",
      "Epoch: 12000 Train Loss: 0.003867 L2-Train Loss: 0.013161\n",
      "Epoch: 13000 Train Loss: 0.003396 L2-Train Loss: 0.013073\n",
      "Epoch: 14000 Train Loss: 0.002323 L2-Train Loss: 0.012417\n",
      "Epoch: 15000 Train Loss: 0.003258 L2-Train Loss: 0.013656\n",
      "Epoch: 16000 Train Loss: 0.002952 L2-Train Loss: 0.013919\n",
      "Epoch: 17000 Train Loss: 0.002723 L2-Train Loss: 0.014444\n",
      "Epoch: 18000 Train Loss: 0.002828 L2-Train Loss: 0.015054\n",
      "Epoch: 19000 Train Loss: 0.002677 L2-Train Loss: 0.015124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████▉                                                          | 3/10 [02:34<05:59, 51.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.002553 L2-Train Loss: 0.015394\n",
      "Epoch: 1 Train Loss: 0.526529 L2-Train Loss: 0.527390\n",
      "Epoch: 1000 Train Loss: 0.100195 L2-Train Loss: 0.102357\n",
      "Epoch: 2000 Train Loss: 0.031253 L2-Train Loss: 0.034647\n",
      "Epoch: 3000 Train Loss: 0.021903 L2-Train Loss: 0.026057\n",
      "Epoch: 4000 Train Loss: 0.019005 L2-Train Loss: 0.023813\n",
      "Epoch: 5000 Train Loss: 0.014227 L2-Train Loss: 0.019627\n",
      "Epoch: 6000 Train Loss: 0.010756 L2-Train Loss: 0.016910\n",
      "Epoch: 7000 Train Loss: 0.008396 L2-Train Loss: 0.014995\n",
      "Epoch: 8000 Train Loss: 0.007164 L2-Train Loss: 0.014333\n",
      "Epoch: 9000 Train Loss: 0.005068 L2-Train Loss: 0.012937\n",
      "Epoch: 10000 Train Loss: 0.003994 L2-Train Loss: 0.012346\n",
      "Epoch: 11000 Train Loss: 0.004635 L2-Train Loss: 0.013401\n",
      "Epoch: 12000 Train Loss: 0.004677 L2-Train Loss: 0.013799\n",
      "Epoch: 13000 Train Loss: 0.002392 L2-Train Loss: 0.011924\n",
      "Epoch: 14000 Train Loss: 0.003079 L2-Train Loss: 0.012978\n",
      "Epoch: 15000 Train Loss: 0.003402 L2-Train Loss: 0.013653\n",
      "Epoch: 16000 Train Loss: 0.002620 L2-Train Loss: 0.013207\n",
      "Epoch: 17000 Train Loss: 0.003020 L2-Train Loss: 0.014059\n",
      "Epoch: 18000 Train Loss: 0.001961 L2-Train Loss: 0.013587\n",
      "Epoch: 19000 Train Loss: 0.003155 L2-Train Loss: 0.015066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▏                                                 | 4/10 [03:25<05:08, 51.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.002437 L2-Train Loss: 0.014797\n",
      "Epoch: 1 Train Loss: 0.517739 L2-Train Loss: 0.518752\n",
      "Epoch: 1000 Train Loss: 0.062038 L2-Train Loss: 0.064453\n",
      "Epoch: 2000 Train Loss: 0.028800 L2-Train Loss: 0.032156\n",
      "Epoch: 3000 Train Loss: 0.017349 L2-Train Loss: 0.021522\n",
      "Epoch: 4000 Train Loss: 0.010284 L2-Train Loss: 0.015095\n",
      "Epoch: 5000 Train Loss: 0.009131 L2-Train Loss: 0.014662\n",
      "Epoch: 6000 Train Loss: 0.007152 L2-Train Loss: 0.013149\n",
      "Epoch: 7000 Train Loss: 0.005757 L2-Train Loss: 0.012307\n",
      "Epoch: 8000 Train Loss: 0.005978 L2-Train Loss: 0.013004\n",
      "Epoch: 9000 Train Loss: 0.005973 L2-Train Loss: 0.013423\n",
      "Epoch: 10000 Train Loss: 0.003398 L2-Train Loss: 0.011472\n",
      "Epoch: 11000 Train Loss: 0.004065 L2-Train Loss: 0.012451\n",
      "Epoch: 12000 Train Loss: 0.003785 L2-Train Loss: 0.012493\n",
      "Epoch: 13000 Train Loss: 0.002427 L2-Train Loss: 0.011915\n",
      "Epoch: 14000 Train Loss: 0.002777 L2-Train Loss: 0.012802\n",
      "Epoch: 15000 Train Loss: 0.002133 L2-Train Loss: 0.012640\n",
      "Epoch: 16000 Train Loss: 0.002791 L2-Train Loss: 0.013694\n",
      "Epoch: 17000 Train Loss: 0.003087 L2-Train Loss: 0.014561\n",
      "Epoch: 18000 Train Loss: 0.002090 L2-Train Loss: 0.013950\n",
      "Epoch: 19000 Train Loss: 0.002935 L2-Train Loss: 0.015038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [04:17<04:17, 51.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.001746 L2-Train Loss: 0.014484\n",
      "Epoch: 1 Train Loss: 0.525312 L2-Train Loss: 0.526402\n",
      "Epoch: 1000 Train Loss: 0.070213 L2-Train Loss: 0.072715\n",
      "Epoch: 2000 Train Loss: 0.023049 L2-Train Loss: 0.026610\n",
      "Epoch: 3000 Train Loss: 0.015177 L2-Train Loss: 0.019555\n",
      "Epoch: 4000 Train Loss: 0.011440 L2-Train Loss: 0.016497\n",
      "Epoch: 5000 Train Loss: 0.008593 L2-Train Loss: 0.014373\n",
      "Epoch: 6000 Train Loss: 0.006846 L2-Train Loss: 0.013162\n",
      "Epoch: 7000 Train Loss: 0.007140 L2-Train Loss: 0.014099\n",
      "Epoch: 8000 Train Loss: 0.004867 L2-Train Loss: 0.012531\n",
      "Epoch: 9000 Train Loss: 0.002431 L2-Train Loss: 0.010528\n",
      "Epoch: 10000 Train Loss: 0.004415 L2-Train Loss: 0.012808\n",
      "Epoch: 11000 Train Loss: 0.002736 L2-Train Loss: 0.011490\n",
      "Epoch: 12000 Train Loss: 0.002741 L2-Train Loss: 0.011863\n",
      "Epoch: 13000 Train Loss: 0.003262 L2-Train Loss: 0.012768\n",
      "Epoch: 14000 Train Loss: 0.002757 L2-Train Loss: 0.012666\n",
      "Epoch: 15000 Train Loss: 0.003174 L2-Train Loss: 0.013588\n",
      "Epoch: 16000 Train Loss: 0.003225 L2-Train Loss: 0.014142\n",
      "Epoch: 17000 Train Loss: 0.002929 L2-Train Loss: 0.014256\n",
      "Epoch: 18000 Train Loss: 0.002639 L2-Train Loss: 0.014372\n",
      "Epoch: 19000 Train Loss: 0.002179 L2-Train Loss: 0.014222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [05:08<03:26, 51.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.002491 L2-Train Loss: 0.014893\n",
      "Epoch: 1 Train Loss: 0.567993 L2-Train Loss: 0.568990\n",
      "Epoch: 1000 Train Loss: 0.079925 L2-Train Loss: 0.082306\n",
      "Epoch: 2000 Train Loss: 0.025492 L2-Train Loss: 0.028892\n",
      "Epoch: 3000 Train Loss: 0.018907 L2-Train Loss: 0.023115\n",
      "Epoch: 4000 Train Loss: 0.012985 L2-Train Loss: 0.017729\n",
      "Epoch: 5000 Train Loss: 0.011441 L2-Train Loss: 0.016853\n",
      "Epoch: 6000 Train Loss: 0.009941 L2-Train Loss: 0.015850\n",
      "Epoch: 7000 Train Loss: 0.008555 L2-Train Loss: 0.015026\n",
      "Epoch: 8000 Train Loss: 0.006087 L2-Train Loss: 0.013190\n",
      "Epoch: 9000 Train Loss: 0.005393 L2-Train Loss: 0.013080\n",
      "Epoch: 10000 Train Loss: 0.004524 L2-Train Loss: 0.012757\n",
      "Epoch: 11000 Train Loss: 0.005631 L2-Train Loss: 0.014139\n",
      "Epoch: 12000 Train Loss: 0.004523 L2-Train Loss: 0.013477\n",
      "Epoch: 13000 Train Loss: 0.004678 L2-Train Loss: 0.014399\n",
      "Epoch: 14000 Train Loss: 0.005090 L2-Train Loss: 0.015201\n",
      "Epoch: 15000 Train Loss: 0.005227 L2-Train Loss: 0.015875\n",
      "Epoch: 16000 Train Loss: 0.003435 L2-Train Loss: 0.014581\n",
      "Epoch: 17000 Train Loss: 0.002489 L2-Train Loss: 0.014255\n",
      "Epoch: 18000 Train Loss: 0.003301 L2-Train Loss: 0.015464\n",
      "Epoch: 19000 Train Loss: 0.002926 L2-Train Loss: 0.015382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [06:00<02:35, 51.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.002646 L2-Train Loss: 0.015201\n",
      "Epoch: 1 Train Loss: 0.615331 L2-Train Loss: 0.616376\n",
      "Epoch: 1000 Train Loss: 0.068378 L2-Train Loss: 0.070870\n",
      "Epoch: 2000 Train Loss: 0.030012 L2-Train Loss: 0.033580\n",
      "Epoch: 3000 Train Loss: 0.021568 L2-Train Loss: 0.025837\n",
      "Epoch: 4000 Train Loss: 0.015823 L2-Train Loss: 0.020715\n",
      "Epoch: 5000 Train Loss: 0.010890 L2-Train Loss: 0.016470\n",
      "Epoch: 6000 Train Loss: 0.009111 L2-Train Loss: 0.015303\n",
      "Epoch: 7000 Train Loss: 0.007683 L2-Train Loss: 0.014482\n",
      "Epoch: 8000 Train Loss: 0.006114 L2-Train Loss: 0.013575\n",
      "Epoch: 9000 Train Loss: 0.006242 L2-Train Loss: 0.014107\n",
      "Epoch: 10000 Train Loss: 0.004260 L2-Train Loss: 0.012741\n",
      "Epoch: 11000 Train Loss: 0.004207 L2-Train Loss: 0.013167\n",
      "Epoch: 12000 Train Loss: 0.003534 L2-Train Loss: 0.012997\n",
      "Epoch: 13000 Train Loss: 0.002810 L2-Train Loss: 0.012529\n",
      "Epoch: 14000 Train Loss: 0.003500 L2-Train Loss: 0.013796\n",
      "Epoch: 15000 Train Loss: 0.002674 L2-Train Loss: 0.013338\n",
      "Epoch: 16000 Train Loss: 0.003986 L2-Train Loss: 0.015008\n",
      "Epoch: 17000 Train Loss: 0.002684 L2-Train Loss: 0.014129\n",
      "Epoch: 18000 Train Loss: 0.002869 L2-Train Loss: 0.014613\n",
      "Epoch: 19000 Train Loss: 0.002056 L2-Train Loss: 0.014056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [06:52<01:42, 51.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.002579 L2-Train Loss: 0.015027\n",
      "Epoch: 1 Train Loss: 0.582170 L2-Train Loss: 0.583265\n",
      "Epoch: 1000 Train Loss: 0.064835 L2-Train Loss: 0.067408\n",
      "Epoch: 2000 Train Loss: 0.025711 L2-Train Loss: 0.029317\n",
      "Epoch: 3000 Train Loss: 0.016924 L2-Train Loss: 0.021323\n",
      "Epoch: 4000 Train Loss: 0.013735 L2-Train Loss: 0.018623\n",
      "Epoch: 5000 Train Loss: 0.009564 L2-Train Loss: 0.015094\n",
      "Epoch: 6000 Train Loss: 0.008284 L2-Train Loss: 0.014359\n",
      "Epoch: 7000 Train Loss: 0.006123 L2-Train Loss: 0.012959\n",
      "Epoch: 8000 Train Loss: 0.004770 L2-Train Loss: 0.012108\n",
      "Epoch: 9000 Train Loss: 0.003603 L2-Train Loss: 0.011428\n",
      "Epoch: 10000 Train Loss: 0.003570 L2-Train Loss: 0.012106\n",
      "Epoch: 11000 Train Loss: 0.003666 L2-Train Loss: 0.012552\n",
      "Epoch: 12000 Train Loss: 0.003236 L2-Train Loss: 0.012648\n",
      "Epoch: 13000 Train Loss: 0.003235 L2-Train Loss: 0.012935\n",
      "Epoch: 14000 Train Loss: 0.002885 L2-Train Loss: 0.013083\n",
      "Epoch: 15000 Train Loss: 0.003368 L2-Train Loss: 0.013847\n",
      "Epoch: 16000 Train Loss: 0.003882 L2-Train Loss: 0.014526\n",
      "Epoch: 17000 Train Loss: 0.002773 L2-Train Loss: 0.014045\n",
      "Epoch: 18000 Train Loss: 0.002559 L2-Train Loss: 0.014172\n",
      "Epoch: 19000 Train Loss: 0.002599 L2-Train Loss: 0.014746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [07:43<00:51, 51.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.003805 L2-Train Loss: 0.016250\n",
      "Epoch: 1 Train Loss: 0.564945 L2-Train Loss: 0.565953\n",
      "Epoch: 1000 Train Loss: 0.069583 L2-Train Loss: 0.072023\n",
      "Epoch: 2000 Train Loss: 0.030198 L2-Train Loss: 0.033531\n",
      "Epoch: 3000 Train Loss: 0.021939 L2-Train Loss: 0.025982\n",
      "Epoch: 4000 Train Loss: 0.016112 L2-Train Loss: 0.020889\n",
      "Epoch: 5000 Train Loss: 0.013112 L2-Train Loss: 0.018609\n",
      "Epoch: 6000 Train Loss: 0.011126 L2-Train Loss: 0.017196\n",
      "Epoch: 7000 Train Loss: 0.008881 L2-Train Loss: 0.015492\n",
      "Epoch: 8000 Train Loss: 0.006019 L2-Train Loss: 0.013110\n",
      "Epoch: 9000 Train Loss: 0.005484 L2-Train Loss: 0.013018\n",
      "Epoch: 10000 Train Loss: 0.005438 L2-Train Loss: 0.013621\n",
      "Epoch: 11000 Train Loss: 0.003846 L2-Train Loss: 0.012475\n",
      "Epoch: 12000 Train Loss: 0.002859 L2-Train Loss: 0.012127\n",
      "Epoch: 13000 Train Loss: 0.003272 L2-Train Loss: 0.013030\n",
      "Epoch: 14000 Train Loss: 0.002382 L2-Train Loss: 0.012583\n",
      "Epoch: 15000 Train Loss: 0.002415 L2-Train Loss: 0.013367\n",
      "Epoch: 16000 Train Loss: 0.002998 L2-Train Loss: 0.014228\n",
      "Epoch: 17000 Train Loss: 0.002446 L2-Train Loss: 0.014111\n",
      "Epoch: 18000 Train Loss: 0.002751 L2-Train Loss: 0.014924\n",
      "Epoch: 19000 Train Loss: 0.002161 L2-Train Loss: 0.014754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [08:34<00:00, 51.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.002146 L2-Train Loss: 0.015142\n",
      "Finish test particle_num=256 cutoff = 100.00 avg_trainloss=2.718e-03 avg_L2_trainloss=1.506e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train Loss: 0.543745 L2-Train Loss: 0.544725\n",
      "Epoch: 1000 Train Loss: 0.069116 L2-Train Loss: 0.071513\n",
      "Epoch: 2000 Train Loss: 0.027308 L2-Train Loss: 0.030709\n",
      "Epoch: 3000 Train Loss: 0.016963 L2-Train Loss: 0.021175\n",
      "Epoch: 4000 Train Loss: 0.012274 L2-Train Loss: 0.017219\n",
      "Epoch: 5000 Train Loss: 0.009528 L2-Train Loss: 0.015159\n",
      "Epoch: 6000 Train Loss: 0.007969 L2-Train Loss: 0.014178\n",
      "Epoch: 7000 Train Loss: 0.006723 L2-Train Loss: 0.013368\n",
      "Epoch: 8000 Train Loss: 0.005752 L2-Train Loss: 0.012953\n",
      "Epoch: 9000 Train Loss: 0.004508 L2-Train Loss: 0.012238\n",
      "Epoch: 10000 Train Loss: 0.003759 L2-Train Loss: 0.011956\n",
      "Epoch: 11000 Train Loss: 0.002941 L2-Train Loss: 0.011586\n",
      "Epoch: 12000 Train Loss: 0.003787 L2-Train Loss: 0.012821\n",
      "Epoch: 13000 Train Loss: 0.002644 L2-Train Loss: 0.012082\n",
      "Epoch: 14000 Train Loss: 0.002361 L2-Train Loss: 0.012273\n",
      "Epoch: 15000 Train Loss: 0.002309 L2-Train Loss: 0.012596\n",
      "Epoch: 16000 Train Loss: 0.002427 L2-Train Loss: 0.013045\n",
      "Epoch: 17000 Train Loss: 0.002135 L2-Train Loss: 0.013190\n",
      "Epoch: 18000 Train Loss: 0.001993 L2-Train Loss: 0.013439\n",
      "Epoch: 19000 Train Loss: 0.001965 L2-Train Loss: 0.013738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████▎                                                                          | 1/10 [00:50<07:38, 50.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.001789 L2-Train Loss: 0.014057\n",
      "Epoch: 1 Train Loss: 0.536944 L2-Train Loss: 0.537993\n",
      "Epoch: 1000 Train Loss: 0.064094 L2-Train Loss: 0.066542\n",
      "Epoch: 2000 Train Loss: 0.026582 L2-Train Loss: 0.030061\n",
      "Epoch: 3000 Train Loss: 0.016685 L2-Train Loss: 0.021018\n",
      "Epoch: 4000 Train Loss: 0.013189 L2-Train Loss: 0.018198\n",
      "Epoch: 5000 Train Loss: 0.009056 L2-Train Loss: 0.014656\n",
      "Epoch: 6000 Train Loss: 0.006912 L2-Train Loss: 0.013095\n",
      "Epoch: 7000 Train Loss: 0.007167 L2-Train Loss: 0.013812\n",
      "Epoch: 8000 Train Loss: 0.004977 L2-Train Loss: 0.012123\n",
      "Epoch: 9000 Train Loss: 0.004037 L2-Train Loss: 0.011711\n",
      "Epoch: 10000 Train Loss: 0.003621 L2-Train Loss: 0.011909\n",
      "Epoch: 11000 Train Loss: 0.003224 L2-Train Loss: 0.012082\n",
      "Epoch: 12000 Train Loss: 0.002628 L2-Train Loss: 0.011822\n",
      "Epoch: 13000 Train Loss: 0.002756 L2-Train Loss: 0.012436\n",
      "Epoch: 14000 Train Loss: 0.002081 L2-Train Loss: 0.012315\n",
      "Epoch: 15000 Train Loss: 0.001984 L2-Train Loss: 0.012504\n",
      "Epoch: 16000 Train Loss: 0.001804 L2-Train Loss: 0.012829\n",
      "Epoch: 17000 Train Loss: 0.001842 L2-Train Loss: 0.013169\n",
      "Epoch: 18000 Train Loss: 0.002043 L2-Train Loss: 0.013908\n",
      "Epoch: 19000 Train Loss: 0.001401 L2-Train Loss: 0.013688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▌                                                                  | 2/10 [01:42<06:48, 51.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.001569 L2-Train Loss: 0.014226\n",
      "Epoch: 1 Train Loss: 0.529683 L2-Train Loss: 0.530670\n",
      "Epoch: 1000 Train Loss: 0.070915 L2-Train Loss: 0.073299\n",
      "Epoch: 2000 Train Loss: 0.024582 L2-Train Loss: 0.028000\n",
      "Epoch: 3000 Train Loss: 0.016231 L2-Train Loss: 0.020410\n",
      "Epoch: 4000 Train Loss: 0.013960 L2-Train Loss: 0.018739\n",
      "Epoch: 5000 Train Loss: 0.010043 L2-Train Loss: 0.015550\n",
      "Epoch: 6000 Train Loss: 0.007944 L2-Train Loss: 0.014049\n",
      "Epoch: 7000 Train Loss: 0.005771 L2-Train Loss: 0.012288\n",
      "Epoch: 8000 Train Loss: 0.004968 L2-Train Loss: 0.012074\n",
      "Epoch: 9000 Train Loss: 0.003627 L2-Train Loss: 0.011309\n",
      "Epoch: 10000 Train Loss: 0.003696 L2-Train Loss: 0.011832\n",
      "Epoch: 11000 Train Loss: 0.002991 L2-Train Loss: 0.011627\n",
      "Epoch: 12000 Train Loss: 0.003032 L2-Train Loss: 0.012183\n",
      "Epoch: 13000 Train Loss: 0.002349 L2-Train Loss: 0.012026\n",
      "Epoch: 14000 Train Loss: 0.002227 L2-Train Loss: 0.012397\n",
      "Epoch: 15000 Train Loss: 0.002372 L2-Train Loss: 0.012997\n",
      "Epoch: 16000 Train Loss: 0.002362 L2-Train Loss: 0.013347\n",
      "Epoch: 17000 Train Loss: 0.001628 L2-Train Loss: 0.013129\n",
      "Epoch: 18000 Train Loss: 0.002018 L2-Train Loss: 0.013916\n",
      "Epoch: 19000 Train Loss: 0.001857 L2-Train Loss: 0.014103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████▉                                                          | 3/10 [02:32<05:56, 50.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.002288 L2-Train Loss: 0.014632\n",
      "Epoch: 1 Train Loss: 0.572114 L2-Train Loss: 0.573094\n",
      "Epoch: 1000 Train Loss: 0.072553 L2-Train Loss: 0.074888\n",
      "Epoch: 2000 Train Loss: 0.025809 L2-Train Loss: 0.029195\n",
      "Epoch: 3000 Train Loss: 0.017067 L2-Train Loss: 0.021261\n",
      "Epoch: 4000 Train Loss: 0.014254 L2-Train Loss: 0.019043\n",
      "Epoch: 5000 Train Loss: 0.011954 L2-Train Loss: 0.017282\n",
      "Epoch: 6000 Train Loss: 0.009913 L2-Train Loss: 0.015758\n",
      "Epoch: 7000 Train Loss: 0.006037 L2-Train Loss: 0.012383\n",
      "Epoch: 8000 Train Loss: 0.004238 L2-Train Loss: 0.011021\n",
      "Epoch: 9000 Train Loss: 0.004031 L2-Train Loss: 0.011407\n",
      "Epoch: 10000 Train Loss: 0.004008 L2-Train Loss: 0.011846\n",
      "Epoch: 11000 Train Loss: 0.003541 L2-Train Loss: 0.011881\n",
      "Epoch: 12000 Train Loss: 0.002890 L2-Train Loss: 0.011621\n",
      "Epoch: 13000 Train Loss: 0.002639 L2-Train Loss: 0.011824\n",
      "Epoch: 14000 Train Loss: 0.002120 L2-Train Loss: 0.011834\n",
      "Epoch: 15000 Train Loss: 0.002220 L2-Train Loss: 0.012420\n",
      "Epoch: 16000 Train Loss: 0.001606 L2-Train Loss: 0.012293\n",
      "Epoch: 17000 Train Loss: 0.002309 L2-Train Loss: 0.013470\n",
      "Epoch: 18000 Train Loss: 0.001903 L2-Train Loss: 0.013491\n",
      "Epoch: 19000 Train Loss: 0.001997 L2-Train Loss: 0.014126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▏                                                 | 4/10 [03:23<05:05, 50.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.002072 L2-Train Loss: 0.014797\n",
      "Epoch: 1 Train Loss: 0.524522 L2-Train Loss: 0.525484\n",
      "Epoch: 1000 Train Loss: 0.081532 L2-Train Loss: 0.083847\n",
      "Epoch: 2000 Train Loss: 0.026721 L2-Train Loss: 0.030231\n",
      "Epoch: 3000 Train Loss: 0.016826 L2-Train Loss: 0.021180\n",
      "Epoch: 4000 Train Loss: 0.013336 L2-Train Loss: 0.018323\n",
      "Epoch: 5000 Train Loss: 0.009694 L2-Train Loss: 0.015334\n",
      "Epoch: 6000 Train Loss: 0.007460 L2-Train Loss: 0.013687\n",
      "Epoch: 7000 Train Loss: 0.006094 L2-Train Loss: 0.012978\n",
      "Epoch: 8000 Train Loss: 0.004665 L2-Train Loss: 0.012181\n",
      "Epoch: 9000 Train Loss: 0.003802 L2-Train Loss: 0.011957\n",
      "Epoch: 10000 Train Loss: 0.003865 L2-Train Loss: 0.012589\n",
      "Epoch: 11000 Train Loss: 0.003039 L2-Train Loss: 0.012253\n",
      "Epoch: 12000 Train Loss: 0.002581 L2-Train Loss: 0.012134\n",
      "Epoch: 13000 Train Loss: 0.002381 L2-Train Loss: 0.012404\n",
      "Epoch: 14000 Train Loss: 0.002684 L2-Train Loss: 0.013102\n",
      "Epoch: 15000 Train Loss: 0.002142 L2-Train Loss: 0.013150\n",
      "Epoch: 16000 Train Loss: 0.001912 L2-Train Loss: 0.013361\n",
      "Epoch: 17000 Train Loss: 0.001822 L2-Train Loss: 0.013669\n",
      "Epoch: 18000 Train Loss: 0.001955 L2-Train Loss: 0.014366\n",
      "Epoch: 19000 Train Loss: 0.002267 L2-Train Loss: 0.015069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [04:14<04:14, 50.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.001991 L2-Train Loss: 0.015146\n",
      "Epoch: 1 Train Loss: 0.526717 L2-Train Loss: 0.527775\n",
      "Epoch: 1000 Train Loss: 0.071172 L2-Train Loss: 0.073600\n",
      "Epoch: 2000 Train Loss: 0.030531 L2-Train Loss: 0.033996\n",
      "Epoch: 3000 Train Loss: 0.019651 L2-Train Loss: 0.023883\n",
      "Epoch: 4000 Train Loss: 0.014736 L2-Train Loss: 0.019656\n",
      "Epoch: 5000 Train Loss: 0.010874 L2-Train Loss: 0.016509\n",
      "Epoch: 6000 Train Loss: 0.008593 L2-Train Loss: 0.014854\n",
      "Epoch: 7000 Train Loss: 0.006985 L2-Train Loss: 0.013862\n",
      "Epoch: 8000 Train Loss: 0.005794 L2-Train Loss: 0.013243\n",
      "Epoch: 9000 Train Loss: 0.005387 L2-Train Loss: 0.013330\n",
      "Epoch: 10000 Train Loss: 0.004046 L2-Train Loss: 0.012384\n",
      "Epoch: 11000 Train Loss: 0.003553 L2-Train Loss: 0.012364\n",
      "Epoch: 12000 Train Loss: 0.003046 L2-Train Loss: 0.012244\n",
      "Epoch: 13000 Train Loss: 0.002821 L2-Train Loss: 0.012586\n",
      "Epoch: 14000 Train Loss: 0.002983 L2-Train Loss: 0.013262\n",
      "Epoch: 15000 Train Loss: 0.002335 L2-Train Loss: 0.013079\n",
      "Epoch: 16000 Train Loss: 0.002156 L2-Train Loss: 0.013180\n",
      "Epoch: 17000 Train Loss: 0.001953 L2-Train Loss: 0.013470\n",
      "Epoch: 18000 Train Loss: 0.001847 L2-Train Loss: 0.013833\n",
      "Epoch: 19000 Train Loss: 0.001849 L2-Train Loss: 0.014188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [05:05<03:23, 50.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.002162 L2-Train Loss: 0.014936\n",
      "Epoch: 1 Train Loss: 0.519167 L2-Train Loss: 0.520195\n",
      "Epoch: 1000 Train Loss: 0.065337 L2-Train Loss: 0.067731\n",
      "Epoch: 2000 Train Loss: 0.029255 L2-Train Loss: 0.032656\n",
      "Epoch: 3000 Train Loss: 0.018375 L2-Train Loss: 0.022643\n",
      "Epoch: 4000 Train Loss: 0.014523 L2-Train Loss: 0.019434\n",
      "Epoch: 5000 Train Loss: 0.010181 L2-Train Loss: 0.015781\n",
      "Epoch: 6000 Train Loss: 0.007472 L2-Train Loss: 0.013727\n",
      "Epoch: 7000 Train Loss: 0.006585 L2-Train Loss: 0.013373\n",
      "Epoch: 8000 Train Loss: 0.005531 L2-Train Loss: 0.012802\n",
      "Epoch: 9000 Train Loss: 0.004395 L2-Train Loss: 0.012263\n",
      "Epoch: 10000 Train Loss: 0.003992 L2-Train Loss: 0.012331\n",
      "Epoch: 11000 Train Loss: 0.004097 L2-Train Loss: 0.012840\n",
      "Epoch: 12000 Train Loss: 0.002674 L2-Train Loss: 0.012072\n",
      "Epoch: 13000 Train Loss: 0.002484 L2-Train Loss: 0.012226\n",
      "Epoch: 14000 Train Loss: 0.002391 L2-Train Loss: 0.012563\n",
      "Epoch: 15000 Train Loss: 0.002063 L2-Train Loss: 0.012861\n",
      "Epoch: 16000 Train Loss: 0.002613 L2-Train Loss: 0.013958\n",
      "Epoch: 17000 Train Loss: 0.002315 L2-Train Loss: 0.014142\n",
      "Epoch: 18000 Train Loss: 0.001805 L2-Train Loss: 0.014171\n",
      "Epoch: 19000 Train Loss: 0.001444 L2-Train Loss: 0.014451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [05:56<02:32, 50.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.001648 L2-Train Loss: 0.015075\n",
      "Epoch: 1 Train Loss: 0.563030 L2-Train Loss: 0.564029\n",
      "Epoch: 1000 Train Loss: 0.064729 L2-Train Loss: 0.067182\n",
      "Epoch: 2000 Train Loss: 0.026219 L2-Train Loss: 0.029679\n",
      "Epoch: 3000 Train Loss: 0.018718 L2-Train Loss: 0.022904\n",
      "Epoch: 4000 Train Loss: 0.011871 L2-Train Loss: 0.016891\n",
      "Epoch: 5000 Train Loss: 0.010403 L2-Train Loss: 0.015986\n",
      "Epoch: 6000 Train Loss: 0.007054 L2-Train Loss: 0.013344\n",
      "Epoch: 7000 Train Loss: 0.005635 L2-Train Loss: 0.012519\n",
      "Epoch: 8000 Train Loss: 0.005279 L2-Train Loss: 0.012757\n",
      "Epoch: 9000 Train Loss: 0.004565 L2-Train Loss: 0.012602\n",
      "Epoch: 10000 Train Loss: 0.002903 L2-Train Loss: 0.011411\n",
      "Epoch: 11000 Train Loss: 0.003692 L2-Train Loss: 0.012670\n",
      "Epoch: 12000 Train Loss: 0.002542 L2-Train Loss: 0.012012\n",
      "Epoch: 13000 Train Loss: 0.002683 L2-Train Loss: 0.012587\n",
      "Epoch: 14000 Train Loss: 0.002642 L2-Train Loss: 0.012840\n",
      "Epoch: 15000 Train Loss: 0.002256 L2-Train Loss: 0.012969\n",
      "Epoch: 16000 Train Loss: 0.001929 L2-Train Loss: 0.013106\n",
      "Epoch: 17000 Train Loss: 0.002006 L2-Train Loss: 0.013588\n",
      "Epoch: 18000 Train Loss: 0.001677 L2-Train Loss: 0.013719\n",
      "Epoch: 19000 Train Loss: 0.001436 L2-Train Loss: 0.013898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [06:47<01:41, 50.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.001685 L2-Train Loss: 0.014626\n",
      "Epoch: 1 Train Loss: 0.540290 L2-Train Loss: 0.541270\n",
      "Epoch: 1000 Train Loss: 0.071450 L2-Train Loss: 0.073858\n",
      "Epoch: 2000 Train Loss: 0.028423 L2-Train Loss: 0.031864\n",
      "Epoch: 3000 Train Loss: 0.019475 L2-Train Loss: 0.023660\n",
      "Epoch: 4000 Train Loss: 0.014841 L2-Train Loss: 0.019788\n",
      "Epoch: 5000 Train Loss: 0.010021 L2-Train Loss: 0.015655\n",
      "Epoch: 6000 Train Loss: 0.007804 L2-Train Loss: 0.014091\n",
      "Epoch: 7000 Train Loss: 0.005849 L2-Train Loss: 0.012793\n",
      "Epoch: 8000 Train Loss: 0.004823 L2-Train Loss: 0.012296\n",
      "Epoch: 9000 Train Loss: 0.004204 L2-Train Loss: 0.012237\n",
      "Epoch: 10000 Train Loss: 0.003539 L2-Train Loss: 0.011868\n",
      "Epoch: 11000 Train Loss: 0.003277 L2-Train Loss: 0.012164\n",
      "Epoch: 12000 Train Loss: 0.003473 L2-Train Loss: 0.012776\n",
      "Epoch: 13000 Train Loss: 0.003073 L2-Train Loss: 0.012818\n",
      "Epoch: 14000 Train Loss: 0.001983 L2-Train Loss: 0.012200\n",
      "Epoch: 15000 Train Loss: 0.002284 L2-Train Loss: 0.012852\n",
      "Epoch: 16000 Train Loss: 0.002400 L2-Train Loss: 0.013384\n",
      "Epoch: 17000 Train Loss: 0.002005 L2-Train Loss: 0.013464\n",
      "Epoch: 18000 Train Loss: 0.001862 L2-Train Loss: 0.013882\n",
      "Epoch: 19000 Train Loss: 0.002119 L2-Train Loss: 0.014485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [07:38<00:50, 50.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.001680 L2-Train Loss: 0.014471\n",
      "Epoch: 1 Train Loss: 0.526394 L2-Train Loss: 0.527357\n",
      "Epoch: 1000 Train Loss: 0.069888 L2-Train Loss: 0.072224\n",
      "Epoch: 2000 Train Loss: 0.029896 L2-Train Loss: 0.033248\n",
      "Epoch: 3000 Train Loss: 0.018052 L2-Train Loss: 0.022214\n",
      "Epoch: 4000 Train Loss: 0.013744 L2-Train Loss: 0.018641\n",
      "Epoch: 5000 Train Loss: 0.010085 L2-Train Loss: 0.015532\n",
      "Epoch: 6000 Train Loss: 0.007782 L2-Train Loss: 0.013779\n",
      "Epoch: 7000 Train Loss: 0.006016 L2-Train Loss: 0.012664\n",
      "Epoch: 8000 Train Loss: 0.004849 L2-Train Loss: 0.012163\n",
      "Epoch: 9000 Train Loss: 0.004156 L2-Train Loss: 0.012038\n",
      "Epoch: 10000 Train Loss: 0.002956 L2-Train Loss: 0.011461\n",
      "Epoch: 11000 Train Loss: 0.002397 L2-Train Loss: 0.011399\n",
      "Epoch: 12000 Train Loss: 0.002396 L2-Train Loss: 0.012076\n",
      "Epoch: 13000 Train Loss: 0.002284 L2-Train Loss: 0.012410\n",
      "Epoch: 14000 Train Loss: 0.002124 L2-Train Loss: 0.012732\n",
      "Epoch: 15000 Train Loss: 0.002347 L2-Train Loss: 0.013405\n",
      "Epoch: 16000 Train Loss: 0.002751 L2-Train Loss: 0.014371\n",
      "Epoch: 17000 Train Loss: 0.002308 L2-Train Loss: 0.014430\n",
      "Epoch: 18000 Train Loss: 0.001802 L2-Train Loss: 0.014367\n",
      "Epoch: 19000 Train Loss: 0.002390 L2-Train Loss: 0.015271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [08:29<00:00, 50.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.001323 L2-Train Loss: 0.014576\n",
      "Finish test particle_num=512 cutoff = 100.00 avg_trainloss=1.857e-03 avg_L2_trainloss=1.459e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train Loss: 0.535964 L2-Train Loss: 0.536977\n",
      "Epoch: 1000 Train Loss: 0.075559 L2-Train Loss: 0.077932\n",
      "Epoch: 2000 Train Loss: 0.026872 L2-Train Loss: 0.030369\n",
      "Epoch: 3000 Train Loss: 0.016721 L2-Train Loss: 0.021044\n",
      "Epoch: 4000 Train Loss: 0.012901 L2-Train Loss: 0.017997\n",
      "Epoch: 5000 Train Loss: 0.010728 L2-Train Loss: 0.016337\n",
      "Epoch: 6000 Train Loss: 0.008256 L2-Train Loss: 0.014460\n",
      "Epoch: 7000 Train Loss: 0.006360 L2-Train Loss: 0.013030\n",
      "Epoch: 8000 Train Loss: 0.005489 L2-Train Loss: 0.012683\n",
      "Epoch: 9000 Train Loss: 0.003835 L2-Train Loss: 0.011582\n",
      "Epoch: 10000 Train Loss: 0.003298 L2-Train Loss: 0.011465\n",
      "Epoch: 11000 Train Loss: 0.003327 L2-Train Loss: 0.011899\n",
      "Epoch: 12000 Train Loss: 0.003046 L2-Train Loss: 0.012132\n",
      "Epoch: 13000 Train Loss: 0.002317 L2-Train Loss: 0.011848\n",
      "Epoch: 14000 Train Loss: 0.002427 L2-Train Loss: 0.012413\n",
      "Epoch: 15000 Train Loss: 0.002288 L2-Train Loss: 0.012724\n",
      "Epoch: 16000 Train Loss: 0.001935 L2-Train Loss: 0.012788\n",
      "Epoch: 17000 Train Loss: 0.001806 L2-Train Loss: 0.013240\n",
      "Epoch: 18000 Train Loss: 0.001558 L2-Train Loss: 0.013473\n",
      "Epoch: 19000 Train Loss: 0.001501 L2-Train Loss: 0.013771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████▎                                                                          | 1/10 [00:50<07:32, 50.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.001525 L2-Train Loss: 0.014195\n",
      "Epoch: 1 Train Loss: 0.548370 L2-Train Loss: 0.549365\n",
      "Epoch: 1000 Train Loss: 0.069172 L2-Train Loss: 0.071567\n",
      "Epoch: 2000 Train Loss: 0.027493 L2-Train Loss: 0.030957\n",
      "Epoch: 3000 Train Loss: 0.018346 L2-Train Loss: 0.022515\n",
      "Epoch: 4000 Train Loss: 0.013804 L2-Train Loss: 0.018683\n",
      "Epoch: 5000 Train Loss: 0.009856 L2-Train Loss: 0.015354\n",
      "Epoch: 6000 Train Loss: 0.008777 L2-Train Loss: 0.014790\n",
      "Epoch: 7000 Train Loss: 0.007052 L2-Train Loss: 0.013605\n",
      "Epoch: 8000 Train Loss: 0.005382 L2-Train Loss: 0.012506\n",
      "Epoch: 9000 Train Loss: 0.004815 L2-Train Loss: 0.012473\n",
      "Epoch: 10000 Train Loss: 0.003607 L2-Train Loss: 0.011812\n",
      "Epoch: 11000 Train Loss: 0.002735 L2-Train Loss: 0.011469\n",
      "Epoch: 12000 Train Loss: 0.002968 L2-Train Loss: 0.012160\n",
      "Epoch: 13000 Train Loss: 0.002573 L2-Train Loss: 0.012222\n",
      "Epoch: 14000 Train Loss: 0.002312 L2-Train Loss: 0.012450\n",
      "Epoch: 15000 Train Loss: 0.001987 L2-Train Loss: 0.012599\n",
      "Epoch: 16000 Train Loss: 0.002098 L2-Train Loss: 0.013105\n",
      "Epoch: 17000 Train Loss: 0.001843 L2-Train Loss: 0.013316\n",
      "Epoch: 18000 Train Loss: 0.002095 L2-Train Loss: 0.013946\n",
      "Epoch: 19000 Train Loss: 0.001406 L2-Train Loss: 0.013716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▌                                                                  | 2/10 [01:41<06:46, 50.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.001772 L2-Train Loss: 0.014478\n",
      "Epoch: 1 Train Loss: 0.538332 L2-Train Loss: 0.539314\n",
      "Epoch: 1000 Train Loss: 0.071593 L2-Train Loss: 0.073963\n",
      "Epoch: 2000 Train Loss: 0.027055 L2-Train Loss: 0.030477\n",
      "Epoch: 3000 Train Loss: 0.016648 L2-Train Loss: 0.020871\n",
      "Epoch: 4000 Train Loss: 0.010972 L2-Train Loss: 0.015863\n",
      "Epoch: 5000 Train Loss: 0.008632 L2-Train Loss: 0.014120\n",
      "Epoch: 6000 Train Loss: 0.007071 L2-Train Loss: 0.013197\n",
      "Epoch: 7000 Train Loss: 0.005142 L2-Train Loss: 0.011819\n",
      "Epoch: 8000 Train Loss: 0.004223 L2-Train Loss: 0.011401\n",
      "Epoch: 9000 Train Loss: 0.003970 L2-Train Loss: 0.011718\n",
      "Epoch: 10000 Train Loss: 0.003087 L2-Train Loss: 0.011193\n",
      "Epoch: 11000 Train Loss: 0.002487 L2-Train Loss: 0.011142\n",
      "Epoch: 12000 Train Loss: 0.002418 L2-Train Loss: 0.011610\n",
      "Epoch: 13000 Train Loss: 0.002296 L2-Train Loss: 0.011927\n",
      "Epoch: 14000 Train Loss: 0.002388 L2-Train Loss: 0.012491\n",
      "Epoch: 15000 Train Loss: 0.002068 L2-Train Loss: 0.012603\n",
      "Epoch: 16000 Train Loss: 0.001985 L2-Train Loss: 0.012982\n",
      "Epoch: 17000 Train Loss: 0.001866 L2-Train Loss: 0.013232\n",
      "Epoch: 18000 Train Loss: 0.001694 L2-Train Loss: 0.013436\n",
      "Epoch: 19000 Train Loss: 0.001717 L2-Train Loss: 0.013789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████▉                                                          | 3/10 [02:32<05:56, 50.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.001213 L2-Train Loss: 0.013757\n",
      "Epoch: 1 Train Loss: 0.538993 L2-Train Loss: 0.539981\n",
      "Epoch: 1000 Train Loss: 0.067558 L2-Train Loss: 0.069955\n",
      "Epoch: 2000 Train Loss: 0.026382 L2-Train Loss: 0.029845\n",
      "Epoch: 3000 Train Loss: 0.018097 L2-Train Loss: 0.022319\n",
      "Epoch: 4000 Train Loss: 0.013406 L2-Train Loss: 0.018323\n",
      "Epoch: 5000 Train Loss: 0.010952 L2-Train Loss: 0.016503\n",
      "Epoch: 6000 Train Loss: 0.009001 L2-Train Loss: 0.015103\n",
      "Epoch: 7000 Train Loss: 0.006796 L2-Train Loss: 0.013493\n",
      "Epoch: 8000 Train Loss: 0.005436 L2-Train Loss: 0.012674\n",
      "Epoch: 9000 Train Loss: 0.004493 L2-Train Loss: 0.012257\n",
      "Epoch: 10000 Train Loss: 0.003893 L2-Train Loss: 0.012214\n",
      "Epoch: 11000 Train Loss: 0.003113 L2-Train Loss: 0.012008\n",
      "Epoch: 12000 Train Loss: 0.002842 L2-Train Loss: 0.012324\n",
      "Epoch: 13000 Train Loss: 0.002394 L2-Train Loss: 0.012308\n",
      "Epoch: 14000 Train Loss: 0.002188 L2-Train Loss: 0.012546\n",
      "Epoch: 15000 Train Loss: 0.001836 L2-Train Loss: 0.012719\n",
      "Epoch: 16000 Train Loss: 0.001512 L2-Train Loss: 0.012891\n",
      "Epoch: 17000 Train Loss: 0.001458 L2-Train Loss: 0.013357\n",
      "Epoch: 18000 Train Loss: 0.001246 L2-Train Loss: 0.013433\n",
      "Epoch: 19000 Train Loss: 0.001348 L2-Train Loss: 0.014093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▏                                                 | 4/10 [03:22<05:03, 50.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.001440 L2-Train Loss: 0.014769\n",
      "Epoch: 1 Train Loss: 0.564315 L2-Train Loss: 0.565284\n",
      "Epoch: 1000 Train Loss: 0.070041 L2-Train Loss: 0.072385\n",
      "Epoch: 2000 Train Loss: 0.026864 L2-Train Loss: 0.030248\n",
      "Epoch: 3000 Train Loss: 0.016828 L2-Train Loss: 0.021059\n",
      "Epoch: 4000 Train Loss: 0.011894 L2-Train Loss: 0.016817\n",
      "Epoch: 5000 Train Loss: 0.008583 L2-Train Loss: 0.014157\n",
      "Epoch: 6000 Train Loss: 0.006706 L2-Train Loss: 0.012860\n",
      "Epoch: 7000 Train Loss: 0.005006 L2-Train Loss: 0.011797\n",
      "Epoch: 8000 Train Loss: 0.004306 L2-Train Loss: 0.011572\n",
      "Epoch: 9000 Train Loss: 0.003337 L2-Train Loss: 0.011218\n",
      "Epoch: 10000 Train Loss: 0.002588 L2-Train Loss: 0.010899\n",
      "Epoch: 11000 Train Loss: 0.002707 L2-Train Loss: 0.011613\n",
      "Epoch: 12000 Train Loss: 0.001790 L2-Train Loss: 0.011193\n",
      "Epoch: 13000 Train Loss: 0.001829 L2-Train Loss: 0.011771\n",
      "Epoch: 14000 Train Loss: 0.001637 L2-Train Loss: 0.012008\n",
      "Epoch: 15000 Train Loss: 0.001397 L2-Train Loss: 0.012187\n",
      "Epoch: 16000 Train Loss: 0.001262 L2-Train Loss: 0.012579\n",
      "Epoch: 17000 Train Loss: 0.001437 L2-Train Loss: 0.013208\n",
      "Epoch: 18000 Train Loss: 0.001359 L2-Train Loss: 0.013601\n",
      "Epoch: 19000 Train Loss: 0.001314 L2-Train Loss: 0.013923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [04:13<04:13, 50.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.000875 L2-Train Loss: 0.013883\n",
      "Epoch: 1 Train Loss: 0.528498 L2-Train Loss: 0.529517\n",
      "Epoch: 1000 Train Loss: 0.067210 L2-Train Loss: 0.069616\n",
      "Epoch: 2000 Train Loss: 0.027049 L2-Train Loss: 0.030494\n",
      "Epoch: 3000 Train Loss: 0.017641 L2-Train Loss: 0.021873\n",
      "Epoch: 4000 Train Loss: 0.012266 L2-Train Loss: 0.017250\n",
      "Epoch: 5000 Train Loss: 0.009533 L2-Train Loss: 0.015180\n",
      "Epoch: 6000 Train Loss: 0.006731 L2-Train Loss: 0.012978\n",
      "Epoch: 7000 Train Loss: 0.005552 L2-Train Loss: 0.012252\n",
      "Epoch: 8000 Train Loss: 0.004316 L2-Train Loss: 0.011642\n",
      "Epoch: 9000 Train Loss: 0.003559 L2-Train Loss: 0.011386\n",
      "Epoch: 10000 Train Loss: 0.003431 L2-Train Loss: 0.011775\n",
      "Epoch: 11000 Train Loss: 0.003031 L2-Train Loss: 0.011934\n",
      "Epoch: 12000 Train Loss: 0.002433 L2-Train Loss: 0.011805\n",
      "Epoch: 13000 Train Loss: 0.001914 L2-Train Loss: 0.011830\n",
      "Epoch: 14000 Train Loss: 0.002383 L2-Train Loss: 0.012704\n",
      "Epoch: 15000 Train Loss: 0.001901 L2-Train Loss: 0.012634\n",
      "Epoch: 16000 Train Loss: 0.001575 L2-Train Loss: 0.012758\n",
      "Epoch: 17000 Train Loss: 0.002022 L2-Train Loss: 0.013586\n",
      "Epoch: 18000 Train Loss: 0.001341 L2-Train Loss: 0.013444\n",
      "Epoch: 19000 Train Loss: 0.001696 L2-Train Loss: 0.014336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [05:03<03:22, 50.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.001408 L2-Train Loss: 0.014512\n",
      "Epoch: 1 Train Loss: 0.523960 L2-Train Loss: 0.524917\n",
      "Epoch: 1000 Train Loss: 0.071418 L2-Train Loss: 0.073756\n",
      "Epoch: 2000 Train Loss: 0.026591 L2-Train Loss: 0.030019\n",
      "Epoch: 3000 Train Loss: 0.017182 L2-Train Loss: 0.021408\n",
      "Epoch: 4000 Train Loss: 0.011866 L2-Train Loss: 0.016758\n",
      "Epoch: 5000 Train Loss: 0.009499 L2-Train Loss: 0.015065\n",
      "Epoch: 6000 Train Loss: 0.007796 L2-Train Loss: 0.013901\n",
      "Epoch: 7000 Train Loss: 0.006437 L2-Train Loss: 0.013108\n",
      "Epoch: 8000 Train Loss: 0.004371 L2-Train Loss: 0.011531\n",
      "Epoch: 9000 Train Loss: 0.004007 L2-Train Loss: 0.011678\n",
      "Epoch: 10000 Train Loss: 0.003554 L2-Train Loss: 0.011685\n",
      "Epoch: 11000 Train Loss: 0.003091 L2-Train Loss: 0.011654\n",
      "Epoch: 12000 Train Loss: 0.002253 L2-Train Loss: 0.011252\n",
      "Epoch: 13000 Train Loss: 0.002152 L2-Train Loss: 0.011676\n",
      "Epoch: 14000 Train Loss: 0.002013 L2-Train Loss: 0.012028\n",
      "Epoch: 15000 Train Loss: 0.002001 L2-Train Loss: 0.012531\n",
      "Epoch: 16000 Train Loss: 0.002094 L2-Train Loss: 0.013058\n",
      "Epoch: 17000 Train Loss: 0.001601 L2-Train Loss: 0.013139\n",
      "Epoch: 18000 Train Loss: 0.001682 L2-Train Loss: 0.013677\n",
      "Epoch: 19000 Train Loss: 0.001247 L2-Train Loss: 0.013536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [05:52<02:30, 50.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.001142 L2-Train Loss: 0.013850\n",
      "Epoch: 1 Train Loss: 0.540268 L2-Train Loss: 0.541276\n",
      "Epoch: 1000 Train Loss: 0.065033 L2-Train Loss: 0.067493\n",
      "Epoch: 2000 Train Loss: 0.027214 L2-Train Loss: 0.030687\n",
      "Epoch: 3000 Train Loss: 0.018739 L2-Train Loss: 0.022921\n",
      "Epoch: 4000 Train Loss: 0.013421 L2-Train Loss: 0.018267\n",
      "Epoch: 5000 Train Loss: 0.011573 L2-Train Loss: 0.016967\n",
      "Epoch: 6000 Train Loss: 0.008234 L2-Train Loss: 0.014262\n",
      "Epoch: 7000 Train Loss: 0.007245 L2-Train Loss: 0.013860\n",
      "Epoch: 8000 Train Loss: 0.005679 L2-Train Loss: 0.012850\n",
      "Epoch: 9000 Train Loss: 0.004913 L2-Train Loss: 0.012710\n",
      "Epoch: 10000 Train Loss: 0.004040 L2-Train Loss: 0.012345\n",
      "Epoch: 11000 Train Loss: 0.003352 L2-Train Loss: 0.012135\n",
      "Epoch: 12000 Train Loss: 0.003379 L2-Train Loss: 0.012651\n",
      "Epoch: 13000 Train Loss: 0.002681 L2-Train Loss: 0.012434\n",
      "Epoch: 14000 Train Loss: 0.002394 L2-Train Loss: 0.012704\n",
      "Epoch: 15000 Train Loss: 0.002107 L2-Train Loss: 0.012931\n",
      "Epoch: 16000 Train Loss: 0.001632 L2-Train Loss: 0.012961\n",
      "Epoch: 17000 Train Loss: 0.002076 L2-Train Loss: 0.013817\n",
      "Epoch: 18000 Train Loss: 0.001539 L2-Train Loss: 0.013716\n",
      "Epoch: 19000 Train Loss: 0.001145 L2-Train Loss: 0.013825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [06:43<01:40, 50.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.001372 L2-Train Loss: 0.014512\n",
      "Epoch: 1 Train Loss: 0.536376 L2-Train Loss: 0.537381\n",
      "Epoch: 1000 Train Loss: 0.071692 L2-Train Loss: 0.074109\n",
      "Epoch: 2000 Train Loss: 0.027956 L2-Train Loss: 0.031465\n",
      "Epoch: 3000 Train Loss: 0.017794 L2-Train Loss: 0.022087\n",
      "Epoch: 4000 Train Loss: 0.013125 L2-Train Loss: 0.018097\n",
      "Epoch: 5000 Train Loss: 0.010108 L2-Train Loss: 0.015716\n",
      "Epoch: 6000 Train Loss: 0.007678 L2-Train Loss: 0.014012\n",
      "Epoch: 7000 Train Loss: 0.005932 L2-Train Loss: 0.012875\n",
      "Epoch: 8000 Train Loss: 0.005082 L2-Train Loss: 0.012483\n",
      "Epoch: 9000 Train Loss: 0.003985 L2-Train Loss: 0.011909\n",
      "Epoch: 10000 Train Loss: 0.003378 L2-Train Loss: 0.011783\n",
      "Epoch: 11000 Train Loss: 0.003438 L2-Train Loss: 0.012331\n",
      "Epoch: 12000 Train Loss: 0.002597 L2-Train Loss: 0.011961\n",
      "Epoch: 13000 Train Loss: 0.002498 L2-Train Loss: 0.012315\n",
      "Epoch: 14000 Train Loss: 0.002208 L2-Train Loss: 0.012674\n",
      "Epoch: 15000 Train Loss: 0.001861 L2-Train Loss: 0.012881\n",
      "Epoch: 16000 Train Loss: 0.001636 L2-Train Loss: 0.013214\n",
      "Epoch: 17000 Train Loss: 0.002020 L2-Train Loss: 0.013957\n",
      "Epoch: 18000 Train Loss: 0.001628 L2-Train Loss: 0.014015\n",
      "Epoch: 19000 Train Loss: 0.001440 L2-Train Loss: 0.014188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [07:31<00:49, 49.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.001271 L2-Train Loss: 0.014395\n",
      "Epoch: 1 Train Loss: 0.555925 L2-Train Loss: 0.556951\n",
      "Epoch: 1000 Train Loss: 0.072536 L2-Train Loss: 0.074928\n",
      "Epoch: 2000 Train Loss: 0.025778 L2-Train Loss: 0.029302\n",
      "Epoch: 3000 Train Loss: 0.016168 L2-Train Loss: 0.020448\n",
      "Epoch: 4000 Train Loss: 0.012530 L2-Train Loss: 0.017513\n",
      "Epoch: 5000 Train Loss: 0.009345 L2-Train Loss: 0.014949\n",
      "Epoch: 6000 Train Loss: 0.007728 L2-Train Loss: 0.013918\n",
      "Epoch: 7000 Train Loss: 0.005354 L2-Train Loss: 0.012145\n",
      "Epoch: 8000 Train Loss: 0.004429 L2-Train Loss: 0.011795\n",
      "Epoch: 9000 Train Loss: 0.003707 L2-Train Loss: 0.011618\n",
      "Epoch: 10000 Train Loss: 0.002728 L2-Train Loss: 0.011182\n",
      "Epoch: 11000 Train Loss: 0.002754 L2-Train Loss: 0.011724\n",
      "Epoch: 12000 Train Loss: 0.002640 L2-Train Loss: 0.011964\n",
      "Epoch: 13000 Train Loss: 0.002137 L2-Train Loss: 0.011940\n",
      "Epoch: 14000 Train Loss: 0.002009 L2-Train Loss: 0.012247\n",
      "Epoch: 15000 Train Loss: 0.001835 L2-Train Loss: 0.012503\n",
      "Epoch: 16000 Train Loss: 0.001953 L2-Train Loss: 0.012998\n",
      "Epoch: 17000 Train Loss: 0.001649 L2-Train Loss: 0.013175\n",
      "Epoch: 18000 Train Loss: 0.001777 L2-Train Loss: 0.013677\n",
      "Epoch: 19000 Train Loss: 0.001489 L2-Train Loss: 0.013842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [08:20<00:00, 50.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Train Loss: 0.001516 L2-Train Loss: 0.014206\n",
      "Finish test particle_num=1024 cutoff = 100.00 avg_trainloss=1.447e-03 avg_L2_trainloss=1.424e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_result = True\n",
    "\n",
    "test_var = 'particle_num'\n",
    "test_values = [64,128,256,512,1024]\n",
    "# test_values = [256]\n",
    "\n",
    "test_time_per_objective = 10\n",
    "\n",
    "result_trainloss_set = []\n",
    "result_L2_trainloss_set = []\n",
    "result_testacc_set = []\n",
    "\n",
    "for test_value in test_values:\n",
    "    globals()[test_var] = test_value\n",
    "    \n",
    "    trainloss_current = []\n",
    "    L2_trainloss_current = []\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for test_number in tqdm(range(test_time_per_objective)):\n",
    "        # Define the loss function and optimizer\n",
    "        net = Net(N=particle_num,cut_off=cut_off,activation_type =activation_type).to(device)\n",
    "\n",
    "        optimizer = FullGradientDescentWithNoisyAndWeightDecay(net.parameters(),particle_num, lr=lr,\n",
    "                                                                weight_decay = weight_decay, noise_scale = sigma)\n",
    "        # optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "        # Train the network\n",
    "        epochs = 20000\n",
    "        train_losses = []\n",
    "        L2_train_losses = []\n",
    "        \n",
    "        net.train()\n",
    "        \n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = net(data)\n",
    "\n",
    "            # loss =  trainloss(output, target).mean()\n",
    "            loss = trainloss(output, target).mean()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * data.size(0) \n",
    "            train_loss /= len(train_loader.dataset)\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "            squared_sum = 0\n",
    "            for param in net.parameters():\n",
    "                squared_sum += torch.sum(param.data ** 2) / particle_num\n",
    "\n",
    "            L2_trian_loss = train_loss + squared_sum.item() * weight_decay\n",
    "            L2_train_losses.append(L2_trian_loss)\n",
    "\n",
    "            if (epoch == 1 or epoch % 1000 == 0) and output_result:\n",
    "                print('Epoch: {} Train Loss: {:.6f} L2-Train Loss: {:.6f}'.format(\n",
    "                    epoch, train_loss, L2_trian_loss))\n",
    "                \n",
    "        trainloss_current.append(train_losses)\n",
    "        L2_trainloss_current.append(L2_train_losses)\n",
    "\n",
    "    result_trainloss_set.append(trainloss_current)\n",
    "    result_L2_trainloss_set.append(L2_trainloss_current)\n",
    "    \n",
    "    avg_trainloss_cur = np.mean(np.array(trainloss_current)[:,-500:])\n",
    "    avg_L2_trainloss =  np.mean(np.array(L2_trainloss_current)[:,-500:])\n",
    "    \n",
    "    print('Finish test %s=%d'%(test_var,globals()[test_var]),'cutoff = %.2f'%(cut_off),\n",
    "          'avg_trainloss=%.3e'%(avg_trainloss_cur), \n",
    "          'avg_L2_trainloss=%.3e'%(avg_L2_trainloss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43b0683",
   "metadata": {},
   "source": [
    "## 数据存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f3ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data save (serialize)\n",
    "\n",
    "with open('data_err_tested_0506_long_MSE.txt','wb') as f:\n",
    "#     pickle.dump({'err_train':err_train,'err_valid':err_valid,'predicts_list':predicts_list},f)  \n",
    "    pickle.dump({'result_trainloss_set':result_trainloss_set, 'result_L2_trainloss_set':result_L2_trainloss_set,'result_testacc_set':result_testacc_set},f)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1044e454",
   "metadata": {},
   "source": [
    "## 可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5de951a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_loss(loss_array, window_size=3):\n",
    "    \"\"\"\n",
    "    Smooths out a 1D array of loss values by replacing each value with the mean\n",
    "    of itself and its neighbors, within a given window size.\n",
    "    \n",
    "    Args:\n",
    "        loss_array (list or numpy array): 1D array of loss values to be smoothed\n",
    "        window_size (int): Size of the window for computing the mean, default is 3\n",
    "        \n",
    "    Returns:\n",
    "        smoothed_loss (numpy array): 1D array of smoothed loss values\n",
    "    \"\"\"\n",
    "    if window_size == 0:\n",
    "        return loss_array\n",
    "    \n",
    "    # Pad the array with zeros on either end to handle edge cases\n",
    "    padded_loss = np.pad(loss_array, (window_size//2, window_size//2), mode='edge')\n",
    "    \n",
    "    # Compute the rolling mean of the padded array\n",
    "    smoothed_loss = np.convolve(padded_loss, np.ones(window_size)/window_size, mode='valid')\n",
    "    \n",
    "    return smoothed_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "407c4d98",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 128, 256, 512, 1024]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x350 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# log 2 loss versus interation numbers, different test var, restore the test info at first\n",
    "\n",
    "# test_var = 'c_cutoff'\n",
    "# test_values = [1,2,5,10,20]\n",
    "\n",
    "test_var = 'particle_num'\n",
    "try:\n",
    "    print(test_values)\n",
    "except:\n",
    "    test_values = [32,64,128,256,512]\n",
    "test_values = [64,128,256,512,1024]\n",
    "\n",
    "log_wise_step = False\n",
    "log_wise_loss = True\n",
    "minus_minimum = False\n",
    "\n",
    "# colors = sns.color_palette(\"Paired\")[1:len(test_values)+1]\n",
    "colors = [sns.xkcd_rgb['pale red'],sns.xkcd_rgb['medium green'], sns.xkcd_rgb['denim blue'],'purple',sns.xkcd_rgb['orange'],'yellow','black','brown']\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(4,3.5))\n",
    "ax1 = fig.add_subplot(111)\n",
    "# ax2 = fig.add_subplot(122)\n",
    "\n",
    "axs = [ax1,ax2]\n",
    "labels = ['train\\ loss', 'L2\\ regularized\\ train\\ loss']\n",
    "\n",
    "for idx, losses in enumerate([result_trainloss_set, result_L2_trainloss_set]):\n",
    "    if idx == 1:\n",
    "        continue\n",
    "    ax = axs[idx]\n",
    "    losses = np.array(losses)\n",
    "\n",
    "    I = np.array(losses).shape[-1]\n",
    "\n",
    "    eps = 1e-6\n",
    "    smooth_window_size = 111\n",
    "    obser_end = int(losses.shape[-1] * 1) \n",
    "\n",
    "    xplot_set = np.arange(I) if log_wise_loss else np.arange(I-1)\n",
    "\n",
    "    for idx_ in range(len(test_values)):\n",
    "\n",
    "        globals()[test_var] = test_values[idx_]\n",
    "\n",
    "        xplot_set = np.log2(np.arange(1,I+1)) if log_wise_step else np.arange(I)\n",
    "\n",
    "        for pending_list in losses[idx_]:\n",
    "\n",
    "            yplot_set = smooth_loss(pending_list, window_size  = smooth_window_size)\n",
    "            yplot_set = np.log2(yplot_set - min(yplot_set) + eps) if minus_minimum else np.log10(yplot_set)\n",
    "\n",
    "            ax.plot(xplot_set[:obser_end],yplot_set[:obser_end],c=colors[idx_],alpha=0.05,zorder = 1)\n",
    "\n",
    "        yplot_set = np.mean(losses[idx_],axis=0)\n",
    "        yplot_set = smooth_loss(yplot_set, window_size  = smooth_window_size)\n",
    "        yplot_set = np.log2(yplot_set - min(yplot_set) + eps) if minus_minimum else np.log10(yplot_set)\n",
    "\n",
    "        ax.plot(xplot_set[:obser_end],yplot_set[:obser_end],linewidth = 2.5,\n",
    "                 zorder = -idx_,c=colors[idx_],label='N = %d'%(test_values[idx_]))\n",
    "\n",
    "    ax.legend(loc = 'upper right',ncol=1)\n",
    "#     ax.set_ylim(-8,-6)\n",
    "    ax.set_xlabel('$log_2 (Step)$') if log_wise_step else plt.xlabel('$Step$',fontsize=13)\n",
    "    ax.set_ylabel('$log_{10} (\\ \\\\frac{1}{N} F_t^N\\ )$',fontsize=13) \n",
    "    \n",
    "plt.savefig('sigmoid_paths.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8e24dc",
   "metadata": {},
   "source": [
    "## Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b4ebec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "trained_losses = []\n",
    "L2_trained_losses = []\n",
    "\n",
    "for idx, test_vulue in enumerate(test_values):\n",
    "    trained_losses_current = []\n",
    "    L2_trained_losses_current = []\n",
    "    \n",
    "    for test_number in range(len(result_trainloss_set[idx])):\n",
    "        trained_losses_current.append(np.mean(result_trainloss_set[idx][test_number][-500:]))\n",
    "        L2_trained_losses_current.append(np.mean(result_L2_trainloss_set[idx][test_number][-500:]))\n",
    "        \n",
    "    trained_losses.append(trained_losses_current)\n",
    "    L2_trained_losses.append(L2_trained_losses_current)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14007ab8",
   "metadata": {},
   "source": [
    "## Fixed alpha regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b5bdf893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 128, 256, 512, 1024]\n"
     ]
    }
   ],
   "source": [
    "test_time_per_objective = len(trained_losses_current)\n",
    "try:\n",
    "    print(test_values)\n",
    "except:\n",
    "    test_values = [32,64,128,256,512]\n",
    "\n",
    "test_values = np.array([test_values])\n",
    "all_data = (np.ones([test_time_per_objective,1]) * test_values).T.reshape(-1)\n",
    "trained_losses = np.array(trained_losses)\n",
    "L2_trained_losses =  np.array(L2_trained_losses)\n",
    "\n",
    "model = linear_model.LinearRegression()\n",
    "alpha = -1.0\n",
    "\n",
    "model.fit(all_data.reshape(-1,1)**(alpha), trained_losses.reshape(-1))\n",
    "w = model.coef_[0]\n",
    "b = model.intercept_\n",
    "\n",
    "model = linear_model.LinearRegression()\n",
    "alpha_l2 = -1.0\n",
    "          \n",
    "model.fit(all_data.reshape(-1,1)**(alpha_l2), L2_trained_losses.reshape(-1))\n",
    "w_l2 = model.coef_[0]\n",
    "b_l2 = model.intercept_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7793453",
   "metadata": {},
   "source": [
    "## Freed alpha fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e53f9023",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  message: Optimization terminated successfully.\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 4.1227823228408375e-06\n",
      "        x: [ 2.510e-02  3.841e+00 -1.057e+00]\n",
      "      nit: 41\n",
      "      jac: [ 3.759e-07 -4.387e-08 -4.599e-07]\n",
      " hess_inv: [[ 2.759e+00  9.840e+02 -8.175e+01]\n",
      "            [ 9.840e+02  5.909e+05 -4.639e+04]\n",
      "            [-8.175e+01 -4.639e+04  3.679e+03]]\n",
      "     nfev: 188\n",
      "     njev: 47\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "def loss(param):\n",
    "    C_1 = param[0]\n",
    "    C_2 = param[1]\n",
    "    alpha = param[2]\n",
    "    return np.mean((L2_trained_losses.reshape(-1) - C_1 - C_2 * all_data ** (alpha) )**2)\n",
    "\n",
    "method = 'BFGS'\n",
    "result = minimize(loss, x0=[0,0,0], method = method)\n",
    "print(result)\n",
    "\n",
    "b_l2, w_l2, alpha_l2 =  result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4d16ced7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x350 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# log 2 terminal loss versus different test var\n",
    "\n",
    "# test_var = 'c_cutoff'\n",
    "# test_values = [1,2,5,10,20]\n",
    "\n",
    "fig = plt.figure(figsize=(4,3.5))\n",
    "ax1 = fig.add_subplot(111)\n",
    "# ax2 = fig.add_subplot(122)\n",
    "\n",
    "axs = [ax1,ax2]\n",
    "labels = ['train\\ loss', 'L2\\ regularized\\ train\\ loss']\n",
    "ws = [w, w_l2]\n",
    "bs = [b, b_l2]\n",
    "alphas = [alpha, alpha_l2]\n",
    "logwise_loss = True\n",
    "\n",
    "for idx, trained_losses_data in enumerate([trained_losses, L2_trained_losses]):\n",
    "    if idx == 1:\n",
    "        continue\n",
    "    ax = axs[idx]\n",
    "    w_plot = ws[idx]\n",
    "    b_plot = bs[idx]\n",
    "    alpha_plot = alphas[idx]\n",
    "    \n",
    "    avg_trained_loss_vec = np.mean(trained_losses_data,axis=1)\n",
    "\n",
    "    all_data = (np.ones([test_time_per_objective,1]) * test_values).T.reshape(-1)\n",
    "    \n",
    "    xplot_all = np.log2(all_data.copy()) if logwise_loss else all_data.copy()\n",
    "    xplot = np.log2(test_values.reshape(-1).copy()) if logwise_loss else test_values.reshape(-1).copy()\n",
    "    yplot_all = np.log10(trained_losses_data.reshape(-1).copy()) if logwise_loss else trained_losses_data.reshape(-1).copy()\n",
    "    yplot =  np.log10(avg_trained_loss_vec.copy())if logwise_loss else avg_trained_loss_vec.copy()\n",
    "    yplot_predict = np.log10(b_plot + w_plot * test_values.reshape(-1) ** (alpha_plot)) if logwise_loss else b_plot + w_plot * test_values.reshape(-1) ** (alpha_plot)\n",
    "    \n",
    "    ax.scatter(xplot_all,yplot_all,color='none',marker='o',edgecolors='k',linewidths=1,label='Experiment result')\n",
    "    ax.plot(xplot,yplot,c='k',marker='s',markersize=8,label='Averange value')\n",
    "    ax.plot(xplot, yplot_predict,'--', marker='o', c= 'grey', label='Fitted curve: \\n    $y= %.3f + %.3f N^{%.3f}$'%(b_plot,w_plot,alpha_plot),alpha=0.6)\n",
    "\n",
    "    ax.legend()\n",
    "    \n",
    "    if logwise_loss:\n",
    "        ax.set_xlabel('$\\log_{2} (N)$',fontsize=13)\n",
    "        ax.set_ylabel('$log_{10} (\\ \\\\frac{1}{N} F_T^N\\ )$',fontsize=13) \n",
    "    else:\n",
    "        ax.set_xlabel('$N$')\n",
    "        ax.set_ylabel('$%s$'%(labels[idx]),) \n",
    "        \n",
    "plt.savefig('sigmoid_losses.pdf', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702c10a7",
   "metadata": {},
   "source": [
    "##  Cross Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83b0b721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1/20] Loss: 0.5577, Acc: 0.8501, Test Loss: 0.1394, Test Acc: 0.9563: 100%|███████| 235/235 [00:17<00:00, 13.64step/s]\n",
      "[2/20] Loss: 0.1231, Acc: 0.9648:  44%|███████████████████▎                        | 103/235 [00:06<00:07, 16.76step/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 86\u001b[0m\n\u001b[0;32m     84\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     85\u001b[0m train_loss, train_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (train_imgs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(processBar):\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():  \u001b[38;5;66;03m# GPU可用\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         train_imgs \u001b[38;5;241m=\u001b[39m train_imgs\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[1;32mF:\\miniconda\\envs\\py310\\lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mF:\\miniconda\\envs\\py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mF:\\miniconda\\envs\\py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mF:\\miniconda\\envs\\py310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mF:\\miniconda\\envs\\py310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mF:\\miniconda\\envs\\py310\\lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mF:\\miniconda\\envs\\py310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mF:\\miniconda\\envs\\py310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mF:\\miniconda\\envs\\py310\\lib\\site-packages\\torchvision\\transforms\\functional.py:174\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    172\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn, optim\n",
    "# from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "from matplotlib.ticker import MaxNLocator\n",
    " \n",
    "# 超参数\n",
    "batch_size = 256  # 批大小\n",
    "learning_rate = 0.0001  # 学习率\n",
    "epochs = 20  # 迭代次数\n",
    "channels = 1  # 图像通道大小\n",
    " \n",
    "# 数据集下载和预处理\n",
    "transform = transforms.Compose([transforms.ToTensor(),  # 将图片转换成PyTorch中处理的对象Tensor,并且进行标准化0-1\n",
    "                                transforms.Normalize([0.5], [0.5])])  # 归一化处理\n",
    "path = './data/'  # 数据集下载后保存的目录\n",
    "# 下载训练集和测试集\n",
    "trainData = datasets.MNIST(path, train=True, transform=transform, download=True)\n",
    "testData = datasets.MNIST(path, train=False, transform=transform)\n",
    "# 处理成data loader\n",
    "trainDataLoader = torch.utils.data.DataLoader(dataset=trainData, batch_size=batch_size, shuffle=True)  # 批量读取并打乱\n",
    "testDataLoader = torch.utils.data.DataLoader(dataset=testData, batch_size=batch_size)\n",
    " \n",
    " \n",
    "# 开始构建cnn模型\n",
    "class cnn(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(cnn, self).__init__()\n",
    "        self.model = torch.nn.Sequential(\n",
    "            # The size of the picture is 28*28\n",
    "            torch.nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    " \n",
    "            # The size of the picture is 14*14\n",
    "            torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            #\n",
    "            # The size of the picture is 7*7\n",
    "            torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            # torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    " \n",
    "            # The size of the picture is 7*7\n",
    "            torch.nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            # torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    " \n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(in_features=7 * 7 * 256, out_features=512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),  # 抑制过拟合 随机丢掉一些节点\n",
    "            torch.nn.Linear(in_features=512, out_features=10),\n",
    "            # torch.nn.Softmax(dim=1) # pytorch的交叉熵函数其实是softmax-log-NLL 所以这里的输出就不需要再softmax了\n",
    "        )\n",
    " \n",
    "    def forward(self, input):\n",
    "        output = self.model(input)\n",
    "        return output\n",
    " \n",
    " \n",
    "# 选择模型\n",
    "model = cnn()\n",
    "# GPU可用时转到cuda上执行\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    " \n",
    "criterion = nn.CrossEntropyLoss()  # 选用交叉熵函数作为损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# optimizer = optim.Adam(model.parameters())\n",
    " \n",
    "# 训练模型并存储训练时的指标\n",
    "epoch = 1\n",
    "history = {'Train Loss': [],\n",
    "           'Test Loss': [],\n",
    "           'Train Acc': [],\n",
    "           'Test Acc': []}\n",
    "for epoch in range(1, epochs+1):\n",
    "    processBar = tqdm(trainDataLoader, unit='step')\n",
    "    model.train(True)\n",
    "    train_loss, train_correct = 0, 0\n",
    "    for step, (train_imgs, labels) in enumerate(processBar):\n",
    " \n",
    "        if torch.cuda.is_available():  # GPU可用\n",
    "            train_imgs = train_imgs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        model.zero_grad()  # 梯度清零\n",
    "        outputs = model(train_imgs)  # 输入训练集\n",
    "        loss = criterion(outputs, labels)  # 计算损失函数\n",
    "        predictions = torch.argmax(outputs, dim=1)  # 得到预测值\n",
    "        correct = torch.sum(predictions == labels)\n",
    "        accuracy = correct / labels.shape[0]  # 计算这一批次的正确率\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step()  # 更新优化器参数\n",
    "        processBar.set_description(\"[%d/%d] Loss: %.4f, Acc: %.4f\" %  # 可视化训练进度条设置\n",
    "                                   (epoch, epochs, loss.item(), accuracy.item()))\n",
    " \n",
    "        # 记录下训练的指标\n",
    "        train_loss = train_loss + loss\n",
    "        train_correct = train_correct + correct\n",
    " \n",
    "        # 当所有训练数据都进行了一次训练后，在验证集进行验证\n",
    "        if step == len(processBar) - 1:\n",
    "            tst_correct, totalLoss = 0, 0\n",
    "            model.train(False)  # 开始测试\n",
    "            model.eval()  # 固定模型的参数并在测试阶段不计算梯度\n",
    "            with torch.no_grad():\n",
    "                for test_imgs, test_labels in testDataLoader:\n",
    "                    if torch.cuda.is_available():\n",
    "                        test_imgs = test_imgs.cuda()\n",
    "                        test_labels = test_labels.cuda()\n",
    "                    tst_outputs = model(test_imgs)\n",
    "                    tst_loss = criterion(tst_outputs, test_labels)\n",
    "                    predictions = torch.argmax(tst_outputs, dim=1)\n",
    " \n",
    "                    totalLoss += tst_loss\n",
    "                    tst_correct += torch.sum(predictions == test_labels)\n",
    " \n",
    "                train_accuracy = train_correct / len(trainDataLoader.dataset)\n",
    "                train_loss = train_loss / len(trainDataLoader)  # 累加loss后除以步数即为平均loss值\n",
    " \n",
    "                test_accuracy = tst_correct / len(testDataLoader.dataset)  # 累加正确数除以样本数即为验证集正确率\n",
    "                test_loss = totalLoss / len(testDataLoader)  # 累加loss后除以步数即为平均loss值\n",
    " \n",
    "                history['Train Loss'].append(train_loss.item())  # 记录loss和acc\n",
    "                history['Train Acc'].append(train_accuracy.item())\n",
    "                history['Test Loss'].append(test_loss.item())\n",
    "                history['Test Acc'].append(test_accuracy.item())\n",
    " \n",
    "                processBar.set_description(\"[%d/%d] Loss: %.4f, Acc: %.4f, Test Loss: %.4f, Test Acc: %.4f\" %\n",
    "                                           (epoch, epochs, train_loss.item(), train_accuracy.item(), test_loss.item(),\n",
    "                                            test_accuracy.item()))\n",
    "    processBar.close()\n",
    "\n",
    "plt.plot(history['Test Loss'], color='red', label='Test Loss')\n",
    "plt.plot(history['Train Loss'], label='Train Loss')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Epoch')\n",
    "plt.xlim([0, epoch])\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train and Test LOSS')\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig('LOSS')\n",
    "plt.show()\n",
    " \n",
    "plt.plot(history['Test Acc'], color='red', label='Test Acc')\n",
    "plt.plot(history['Train Acc'], label='Train Acc')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Epoch')\n",
    "plt.xlim([0, epoch])\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Test ACC')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig('ACC')\n",
    "plt.show()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4169eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a73599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14636f97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ae6925b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (128) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 54\u001b[0m\n\u001b[0;32m     52\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     53\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m---> 54\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     56\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mF:\\miniconda\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mF:\\miniconda\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mF:\\miniconda\\envs\\py310\\lib\\site-packages\\torch\\nn\\functional.py:3294\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3292\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3294\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[1;32mF:\\miniconda\\envs\\py310\\lib\\site-packages\\torch\\functional.py:74\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (128) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define the batch size for training and testing\n",
    "batch_size = 128\n",
    "\n",
    "# Define the transformation to be applied to the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Define the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the neural network model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the neural network model\n",
    "model = Net()\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(\"Epoch {} - Training loss: {}\".format(epoch+1, running_loss/len(train_loader)))\n",
    "\n",
    "# Test the model\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(\"Accuracy on the test set: {:.2f}%\".format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de339a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "22f67127",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(N=1,cut_off=cut_off,activation_type ='ReLU').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2115deb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.Tensor([[0,0],[0,0]]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b5ce203f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0766],\n",
       "        [1.0766]], device='cuda:0', grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "088091d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1.], device='cuda:0')"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.sin(2 * np.pi * data[:,0]) + torch.cos(2 * np.pi * data[:,1])\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3c5471b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = net(data).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "83655b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0059, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = ((predict - target) **2).mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "3d134d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1edbb2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2332],\n",
       "        [1.2332]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fc1(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1a85cc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = target-net(data).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0a020ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0766, -0.0766], device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c9c2ff19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.3973, 0.3350]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1.2332], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "layer_params = net.fc1.parameters()\n",
    "\n",
    "# Print the parameters\n",
    "for param in layer_params:\n",
    "    print(param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "839cefd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.8730]], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "layer_params = net.fc2.parameters()\n",
    "\n",
    "# Print the parameters\n",
    "for param in layer_params:\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "138c9ba5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient for parameter fc1.weight:\n",
      "tensor([[0., 0.]], device='cuda:0')\n",
      "Gradient for parameter fc1.bias:\n",
      "tensor([0.1338], device='cuda:0')\n",
      "Gradient for parameter fc2.weight:\n",
      "tensor([[0.1890]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"Gradient for parameter {name}:\")\n",
    "        print(param.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
